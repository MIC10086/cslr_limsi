{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all helper functions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.models.data_utils import *\n",
    "from src.models.model_utils import *\n",
    "from src.models.train_model import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info:\n",
    "\n",
    "### Code structure\n",
    "- Helper functions are stored in src/models\n",
    "- Data is stored in data/processed\n",
    "\n",
    "### Corpora and annotation\n",
    "#### Dicta-Sign\n",
    "Annotations include (see more detail in the ortolang repo):\n",
    "* **fls**: fully-lexical signs, encoded as categorical values (gloss indices)\n",
    "* **PT**:    pointing signs, encoded as binary\n",
    "* **PT_PRO1**, **PT_PRO2**, **PT_PRO3**, **PT_LOC**, **PT_DET**, **PT_LBUOY**, **PT_BUOY**: sub-categories for pointing signs, encoded as binary\n",
    "* **DS**:    depicting signs, encoded as binary\n",
    "* **DSA**, **DSG**, **DSL**, **DSM**, **DSS**, **DST**, **DSX**: sub-categories for depicting signs, encoded as binary\n",
    "* **FBUOY**: fragment buoys, encoded as binary\n",
    "* **N**:     numbering signs, encoded as binary\n",
    "* **FS**:    fingerspelling signs, encoded as binary\n",
    "\n",
    "#### NCSLGR\n",
    "Annotations include (all data is binary):\n",
    "* **lexical_with_ns_not_fs**: lexical signs, including numbering signs but excluding fingerspelling signs\n",
    "* **fingerspelling**, **fingerspelled_loan_signs**: fingerspelling signs, finglerspelling loan signs\n",
    "* **IX_1p**, **IX_2p**, **IX_3p**, **IX_loc**: sub-categories for pointing signs\n",
    "* **POSS**, **SELF**: possessive pronouns\n",
    "* **DCL**, **LCL**, **SCL**, **BCL**, **ICL**, **BPCL**, **PCL**: sub-categories for classifier signs (i.e. depicting signs)\n",
    "* **gesture**: culturally shared gestures\n",
    "* **part_indef**\n",
    "* **other**\n",
    "\n",
    "### Type of input features\n",
    "Originally, this code was designed around preprocessed features for each frame. Possible features types are : \n",
    "- **2Draw**\n",
    "- **2Draw_HS**\n",
    "- **2Draw_HS_noOP**\n",
    "- **2Draw_noHands**\n",
    "- **2Dfeatures**\n",
    "- **2Dfeatures_HS**\n",
    "- **2Dfeatures_HS_noOP**\n",
    "- **2Dfeatures_noHands**\n",
    "- **3Draw**\n",
    "- **3Draw_HS**\n",
    "- **3Draw_HS_noOP**\n",
    "- **3Draw_noHands**\n",
    "- **3Dfeatures**\n",
    "- **3Dfeatures_HS**\n",
    "- **3Dfeatures_HS_noOP**\n",
    "- **3Dfeatures_noHands**\n",
    "\n",
    "which correspond to 2D or 3D data, raw OpenPose or preprocessed body and face data, including or excluding hand shape estimates, including or excluding OpenPose hand data.\n",
    "\n",
    "The main data function `get_data_concatenated` requires a features dictionary, which can be obtained with the `getFeaturesDict` function.\n",
    "\n",
    "Recently, we also added direct image input to the model, but it has not been tested thoroughly.\n",
    "\n",
    "### Model outputs\n",
    "<a id='list_examples'></a>\n",
    "\n",
    "#### A unique output (`output_form='sign_types'`)\n",
    "This is the setting that was tested thoroughly for the thesis manuscript. This setting can be used when:\n",
    "- one wants to recognize a binary linguistic descriptor\n",
    "    - **[Example 1](#ex1)**: Y1 = [other, depicting signs]\n",
    "    - **[Example 1 bis](#ex1bis)**: Y1bis = [other, depicting signs of type A and G assembled into one category]\n",
    "    - **[Example 2](#ex2)**: Y2 = [other, lexical signs (all)]\n",
    "- one wants to recognize a certain number of lexical signs\n",
    "    - **[Example 2 bis](#ex2bis)**: Y2bis = [other, lexical signs (indices 43015, 43038, 42318, 43357, 43116, 42719)] (binary output)\n",
    "    - **[Example 2 ter](#ex2ter)**: Y2ter = [other, lexical sign 43015, lexical sign 43038, lexical sign 42318, lexical sign 43357, lexical sign 43116, lexical sign 42719] (categorical output)\n",
    "- one wants to recognize sign types as per Yanovich et. al. (the most probable sign type for each frame)\n",
    "    - **[Example 3](#ex3)**: Y3 = [other, lexical signs,  pointing signs, depicting signs, fragment buoys]. \n",
    "\n",
    "#### Multiple mixed output (`output_form='mixed'`)\n",
    "This setting was not tested thoroughly. This setting can be used when:\n",
    "- one wants to recognize several (N) mixed linguistic descriptors in parallel, possibly simultaneously true. Each descriptor includes a 'garbage/other' class.\n",
    "    - **[Example 4](#ex4)** (N = 4) : Y4 = [Y4_1 : lexical signs (categorical with 6 different signs), Y4_2 : pointing signs, Y4_3 : depicting signs, Y4_4 : fragment buoys]\n",
    "    - **[Example 5](#ex5)** (N = 2) : Y5 = [Y5_1 : pointing signs to PRO1/2/3, Y5_2 : lexical signs (all)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help on a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_raw_annotation_from_file in module src.models.data_utils:\n",
      "\n",
      "get_raw_annotation_from_file(corpus, from_notebook=False)\n",
      "    Gets raw annotation from data file\n",
      "    \n",
      "    Inputs:\n",
      "        corpus: 'DictaSign' or 'NCSLGR'\n",
      "        from_notebook: True if used in Jupyter notebook\n",
      "    \n",
      "    Outputs:\n",
      "        Annotation data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help(function_name)\n",
    "# For instance:\n",
    "\n",
    "help(get_raw_annotation_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for data handling (`data_utils.py`): `get_data_concatenated`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to extract data in usable format for training.\n",
    "\n",
    "`get_data_concatenated` returns [X_features, X_frames], Y, idx_trueData or [X_features, X_frames], Y (depending on return_idx_trueData)\n",
    "\n",
    "#### Outputs\n",
    "- **X_features** is a numpy array of size [1, total_time_steps, features_number] containing all retained preprocessed features for all retained frames\n",
    "- **X_frames** is simply a list of paths for all retained frames (frames cannot be stored in memory directly, and will have to be read during training thanks to frames paths)\n",
    "- **Y** is the annotation data (i.e. ground truth data) in the desired format\n",
    "\n",
    "#### Inputs\n",
    "- **corpus** (string)\n",
    "- **output_form**:\n",
    "    - 'mixed' if different and separated Outputs\n",
    "    - 'sign_types' if annotation is only a binary matrix of sign types\n",
    "- **types**: a list of lists of original names that are used to compose final outputs\n",
    "- **nonZero**: a list of lists of nonzero values to consider. If 4 outputs with all nonZero values should be considered, nonZero=[[],[],[],[]]\n",
    "- **binary**: only considered when output_form=mixed. It's a list (True/False) indicating whether the values should be categorical or binary\n",
    "- **features_dict**: a dictionary indication which features to keep ; e.g.: {'features_HS':np.arange(0, 420), 'features_HS_norm':np.array([]), 'raw':np.array([]), 'raw_norm':np.array([])}\n",
    "- **preloaded_features**: if features are already loaded, in the format of a list (features for each video)\n",
    "- **provided_annotation**: raw annotation data (not needed)\n",
    "- **video_indices**: numpy array for a list of videos\n",
    "- **separation**: in order to separate consecutive videos\n",
    "- **from_notebook**: if notebook script, data is in parent folder\n",
    "- **return_idx_trueData**: if True, returns a binary vector with 0 where separations are\n",
    "- **features_type**: 'features', 'frames', 'both'            \n",
    "- **frames_path_before_video**: video frames are supposed to be in folders, like '/localHD/DictaSign/convert/img/DictaSign_lsf_S7_T2_A10',\n",
    "- **empty_image_path**: path of a white frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for model handling (`model_utils.py`): `get_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to obtain the Keras model.\n",
    "\n",
    "`get_model` returns a Keras model\n",
    "\n",
    "#### Outputs\n",
    "- a Keras model\n",
    "\n",
    "#### Inputs\n",
    "- **output_names**: list of outputs (strings)\n",
    "- **output_classes**: list of number of classes of each output type\n",
    "- **output_weights**: list of weights for each_output\n",
    "- **conv** (bool): if True, applies convolution on input\n",
    "- **conv_filt**: number of convolution filters\n",
    "- **conv_ker**: size of convolution kernel\n",
    "- **conv_strides**: size of convolution strides\n",
    "- **rnn_number**: number of recurrent layers\n",
    "- **rnn_type**: type of recurrent layers (string)\n",
    "- **rnn_hidden_units**: number of hidden units\n",
    "- **dropout**: how much dropout (0 to 1)\n",
    "- **att_in_rnn**: if True, applies attention layer before recurrent layers\n",
    "- **att_in_rnn_single**: single (shared) attention layer or not\n",
    "- **att_in_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **att_out_rnn**: if True, applies attention layer after recurrent layers\n",
    "- **att_out_rnn_single**: single (shared) attention layer or not\n",
    "- **att_out_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **rnn_return_sequences**: if False, only last timestep of recurrent layers is returned\n",
    "- **classif_local** (bool): whether classification is for each timestep (local) of globally for the sequence\n",
    "- **mlp_layers_number**: number of additional dense layers\n",
    "- **mlp_layers_size**: size of additional dense layers\n",
    "- **optimizer**: gradient optimizer type (string)\n",
    "- **learning_rate**: learning rate (float)\n",
    "- **time_steps**: length of sequences (int)\n",
    "- **features_number**: number of features (int)\n",
    "- **features_type**: 'features' (1D vector of features), 'frames' (for a CNN processing) or 'both'\n",
    "- **img_height** and **img_width**: size of CNN input\n",
    "- **cnnType**: 'resnet', 'vgg' or 'mobilenet'\n",
    "- **cnnFirstTrainedLayer**: index of first trainable layer in CNN (int)\n",
    "- **cnnReduceDim**: if greater than 0, size of CNN flattened output is reduced to cnnReduceDim\n",
    "- **print_summary** (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut: script to recognize a unique output on DictaSign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use `python src/recognitionUniqueDictaSign.py`\n",
    "\n",
    "Provided help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: recognitionUniqueDictaSign.py [-h] [--outputName OUTPUTNAME]\r\n",
      "                                     [--flsBinary {0,1}]\r\n",
      "                                     [--flsKeep [FLSKEEP [FLSKEEP ...]]]\r\n",
      "                                     [--comment COMMENT]\r\n",
      "                                     [--videoSplitMode {manual,auto}]\r\n",
      "                                     [--fractionValid FRACTIONVALID]\r\n",
      "                                     [--fractionTest FRACTIONTEST]\r\n",
      "                                     [--signerIndependent {0,1}]\r\n",
      "                                     [--taskIndependent {0,1}]\r\n",
      "                                     [--excludeTask9 {0,1}]\r\n",
      "                                     [--tasksTrain [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                     [--tasksValid [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                     [--tasksTest [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                     [--signersTrain [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                     [--signersValid [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                     [--signersTest [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                     [--idxTrainBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                     [--idxValidBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                     [--idxTestBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                     [--randSeed RANDSEED]\r\n",
      "                                     [--weightCorrection WEIGHTCORRECTION]\r\n",
      "                                     [--inputType {2Draw,2Draw_HS,2Draw_HS_noOP,2Draw_noHands,2Dfeatures,2Dfeatures_HS,2Dfeatures_HS_noOP,2Dfeatures_noHands,3Draw,3Draw_HS,3Draw_HS_noOP,3Draw_noHands,3Dfeatures,3Dfeatures_HS,3Dfeatures_HS_noOP,3Dfeatures_noHands,none}]\r\n",
      "                                     [--inputNormed {0,1}]\r\n",
      "                                     [--inputFeaturesFrames {features,frames,both}]\r\n",
      "                                     [--imgWidth {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}]\r\n",
      "                                     [--imgHeight {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}]\r\n",
      "                                     [--cnnType {resnet,vgg,mobilenet}]\r\n",
      "                                     [--cnnFirstTrainedLayer CNNFIRSTTRAINEDLAYER]\r\n",
      "                                     [--cnnReduceDim CNNREDUCEDIM]\r\n",
      "                                     [--seqLength SEQLENGTH]\r\n",
      "                                     [--batchSize BATCHSIZE] [--epochs EPOCHS]\r\n",
      "                                     [--separation SEPARATION]\r\n",
      "                                     [--dropout DROPOUT]\r\n",
      "                                     [--rnnNumber RNNNUMBER]\r\n",
      "                                     [--rnnHiddenUnits RNNHIDDENUNITS]\r\n",
      "                                     [--mlpLayersNumber MLPLAYERSNUMBER]\r\n",
      "                                     [--convolution {0,1}]\r\n",
      "                                     [--convFilt CONVFILT]\r\n",
      "                                     [--convFiltSize CONVFILTSIZE]\r\n",
      "                                     [--learningRate LEARNINGRATE]\r\n",
      "                                     [--optimizer {rms,ada,sgd}]\r\n",
      "                                     [--earlyStopping {0,1}]\r\n",
      "                                     [--redLrOnPlat {0,1}]\r\n",
      "                                     [--redLrMonitor REDLRMONITOR]\r\n",
      "                                     [--redLrMonitorMode {min,max}]\r\n",
      "                                     [--redLrPatience REDLRPATIENCE]\r\n",
      "                                     [--redLrFactor REDLRFACTOR]\r\n",
      "                                     [--saveModel {no,best,all}]\r\n",
      "                                     [--saveBestMonitor SAVEBESTMONITOR]\r\n",
      "                                     [--saveBestMonMode {min,max}]\r\n",
      "                                     [--saveGlobalresults SAVEGLOBALRESULTS]\r\n",
      "                                     [--savePredictions SAVEPREDICTIONS]\r\n",
      "                                     [--fromNotebook {0,1}]\r\n",
      "                                     [--stepWolf {rms,ada,sgd}]\r\n",
      "\r\n",
      "Trains a Keras-TF model for the recognition of a unique type of annotation, on\r\n",
      "the DictaSign-LSF-v2 corpus\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --outputName OUTPUTNAME\r\n",
      "                        The output type that the model is trained to recognize\r\n",
      "  --flsBinary {0,1}     If the output is FLS, if seen as binary\r\n",
      "  --flsKeep [FLSKEEP [FLSKEEP ...]]\r\n",
      "                        If the output is FLS, list of FLS indices to consider\r\n",
      "  --comment COMMENT     A comment to describe this run\r\n",
      "  --videoSplitMode {manual,auto}\r\n",
      "                        Split mode for videos (auto or manually specified)\r\n",
      "  --fractionValid FRACTIONVALID\r\n",
      "                        Fraction of valid data wrt total (if auto split mode)\r\n",
      "  --fractionTest FRACTIONTEST\r\n",
      "                        Fraction of test data wrt total (if auto split mode)\r\n",
      "  --signerIndependent {0,1}\r\n",
      "                        Signer independent train/valid/test random shuffle\r\n",
      "  --taskIndependent {0,1}\r\n",
      "                        Task independent train/valid/test random shuffle\r\n",
      "  --excludeTask9 {0,1}  Whether to exclude task 9\r\n",
      "  --tasksTrain [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Training task indices\r\n",
      "  --tasksValid [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Validation task indices\r\n",
      "  --tasksTest [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Test task indices\r\n",
      "  --signersTrain [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Training signer indices\r\n",
      "  --signersValid [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Validation signer indices\r\n",
      "  --signersTest [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Test signer indices\r\n",
      "  --idxTrainBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --idxValidBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --idxTestBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --randSeed RANDSEED   Random seed (numpy)\r\n",
      "  --weightCorrection WEIGHTCORRECTION\r\n",
      "                        Correction for data imbalance (from 0 (no correction)\r\n",
      "                        to 1)\r\n",
      "  --inputType {2Draw,2Draw_HS,2Draw_HS_noOP,2Draw_noHands,2Dfeatures,2Dfeatures_HS,2Dfeatures_HS_noOP,2Dfeatures_noHands,3Draw,3Draw_HS,3Draw_HS_noOP,3Draw_noHands,3Dfeatures,3Dfeatures_HS,3Dfeatures_HS_noOP,3Dfeatures_noHands,none}\r\n",
      "                        Type of features\r\n",
      "  --inputNormed {0,1}   If features are normed\r\n",
      "  --inputFeaturesFrames {features,frames,both}\r\n",
      "                        Features type\r\n",
      "  --imgWidth {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}\r\n",
      "                        CNN width\r\n",
      "  --imgHeight {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}\r\n",
      "                        CNN height\r\n",
      "  --cnnType {resnet,vgg,mobilenet}\r\n",
      "                        CNN type\r\n",
      "  --cnnFirstTrainedLayer CNNFIRSTTRAINEDLAYER\r\n",
      "                        Index of first trained layer in CNN\r\n",
      "  --cnnReduceDim CNNREDUCEDIM\r\n",
      "                        If greater than 0, the reduced dimension of CNN output\r\n",
      "                        vector\r\n",
      "  --seqLength SEQLENGTH\r\n",
      "                        Length of sequences\r\n",
      "  --batchSize BATCHSIZE\r\n",
      "                        Batch size\r\n",
      "  --epochs EPOCHS       Number of epochs\r\n",
      "  --separation SEPARATION\r\n",
      "                        Separation between videos\r\n",
      "  --dropout DROPOUT     Dropout (0 to 1)\r\n",
      "  --rnnNumber RNNNUMBER\r\n",
      "                        Number of RNN layers\r\n",
      "  --rnnHiddenUnits RNNHIDDENUNITS\r\n",
      "                        Number of hidden units in RNN\r\n",
      "  --mlpLayersNumber MLPLAYERSNUMBER\r\n",
      "                        Number MLP layers after RNN\r\n",
      "  --convolution {0,1}   Whether to use a conv. layer\r\n",
      "  --convFilt CONVFILT   Number of convolution kernels\r\n",
      "  --convFiltSize CONVFILTSIZE\r\n",
      "                        Size of convolution kernels\r\n",
      "  --learningRate LEARNINGRATE\r\n",
      "                        Learning rate\r\n",
      "  --optimizer {rms,ada,sgd}\r\n",
      "                        Training optimizer\r\n",
      "  --earlyStopping {0,1}\r\n",
      "                        Early stopping\r\n",
      "  --redLrOnPlat {0,1}   Reduce l_rate on plateau\r\n",
      "  --redLrMonitor REDLRMONITOR\r\n",
      "                        Metric for l_rate reduction\r\n",
      "  --redLrMonitorMode {min,max}\r\n",
      "                        Mode for l_rate reduction\r\n",
      "  --redLrPatience REDLRPATIENCE\r\n",
      "                        Patience before l_rate reduc\r\n",
      "  --redLrFactor REDLRFACTOR\r\n",
      "                        Factor for each l_rate reduc\r\n",
      "  --saveModel {no,best,all}\r\n",
      "                        Whether to save only best model, or all, or none\r\n",
      "  --saveBestMonitor SAVEBESTMONITOR\r\n",
      "                        What metric to decide best model\r\n",
      "  --saveBestMonMode {min,max}\r\n",
      "                        Mode to define best\r\n",
      "  --saveGlobalresults SAVEGLOBALRESULTS\r\n",
      "                        Where to save global results\r\n",
      "  --savePredictions SAVEPREDICTIONS\r\n",
      "                        Where to save predictions\r\n",
      "  --fromNotebook {0,1}  When the script is run from a jupyter notebook\r\n",
      "  --stepWolf {rms,ada,sgd}\r\n",
      "                        Step between Wolf metric eval points\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/recognitionUniqueDictaSign.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building data and model together, manually, then training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 66\n",
      "Valid: 10\n",
      "Test: 18\n",
      "Total: 94\n",
      "[46  8 27 21 55 60 34 40 42 23 15 59 13 14 65 52  3 24 30 64 50 19 35 16\n",
      " 32  6 33 49  7 47 36  0 38 22 58 62 17 25 41 11 48 43 18 10 39 54 20  5\n",
      " 61 63  1 53 28 26  2 31  4 57 51 29 45 56 12 37 44  9]\n"
     ]
    }
   ],
   "source": [
    "# let us split train/valid/test videos\n",
    "# In this case we split by signers in a manual fashion\n",
    "\n",
    "idxTrain, idxValid, idxTest = getVideoIndicesSplitDictaSign(tasksTrain=[],\n",
    "                                                            tasksValid=[],\n",
    "                                                            tasksTest=[],\n",
    "                                                            signersTrain=[0,1,2,3,4,5,6,7,8,9],\n",
    "                                                            signersValid=[10,11,12],\n",
    "                                                            signersTest=[13,14,15],\n",
    "                                                            excludeTask9=False,\n",
    "                                                            videoSplitMode='manual',\n",
    "                                                            checkSplits=True,\n",
    "                                                            checkSets=True,\n",
    "                                                            from_notebook=True)\n",
    "print(idxTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 32\n",
      "Valid: 3\n",
      "Test: 2\n",
      "Total: 37\n",
      "[14 22 18 29 31  0 19 24 11 21  5 12 16 26 30  2  1  7 27  3 10 15 17 28\n",
      " 25 13 23  6  4 20  8  9]\n"
     ]
    }
   ],
   "source": [
    "# for the below examples to run faster, let us select only a part of all signers\n",
    "idxTrain, idxValid, idxTest = getVideoIndicesSplitDictaSign(tasksTrain=[],\n",
    "                                                            tasksValid=[],\n",
    "                                                            tasksTest=[],\n",
    "                                                            signersTrain=[0,1,2,3],\n",
    "                                                            signersValid=[10],\n",
    "                                                            signersTest=[13],\n",
    "                                                            excludeTask9=False,\n",
    "                                                            videoSplitMode='manual',\n",
    "                                                            checkSplits=True,\n",
    "                                                            checkSets=True,\n",
    "                                                            from_notebook=True)\n",
    "print(idxTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features_HS': array([], dtype=float64), 'features_HS_norm': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419]), 'raw': array([], dtype=float64), 'raw_norm': array([], dtype=float64), '2Dfeatures': array([], dtype=float64), '2Dfeatures_norm': array([], dtype=float64)}\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "# Getting a dictionary for desired preprocessed features\n",
    "# In this case we ask for normalized 3Dfeatures_HS (this correspond to a total number of 420 features):\n",
    "\n",
    "features_dict, features_number = getFeaturesDict(inputType='3Dfeatures_HS', inputNormed=True)\n",
    "\n",
    "print(features_dict)\n",
    "print(features_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First category of examples: `output_form = 'sign_types'` (examples 1 to 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1'></a>\n",
    "#### ex. 1 ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_1, X_frames_train_1], Y_train_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_1, X_frames_valid_1], Y_valid_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "(1, 371696, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1.shape)\n",
    "print(X_frames_train_1.shape)\n",
    "print(Y_train_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, X_feat is a big matrix storing all 420 preprocessed features for each frame. We can print the first 15 features for frame number 192:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.96723568e-01 5.23387462e-05 7.83663709e-05 2.20888451e-05\n",
      " 2.67234395e-06 6.94834671e-05 3.75398144e-04 2.81455839e-04\n",
      " 1.08070999e-05 1.25116358e-05 1.38831223e-04 3.05948500e-03\n",
      " 4.43404042e-06 3.89795568e-05 2.34687264e-04]\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1[0,192,0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localHD/DictaSign/convert/img/DictaSign_lsf_S2_T9_A11_front/00193.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_frames_train_1[192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3571\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# First depicting frame:\n",
    "i_DS_one = np.where(Y_train_1[0,:,1]==1)[0][0]\n",
    "print(i_DS_one)\n",
    "print(Y_train_1[0,i_DS_one,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_1_features = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100, 224, 224, 3) 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 2048)         23587712  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 110)          925760    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 24,586,734\n",
      "Trainable params: 5,464,686\n",
      "Non-trainable params: 19,122,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only frames as input:\n",
    "model_1_frames = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 200)     252200      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 2048)    23587712    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_frames (Concaten (None, 100, 2248)    0           conv1d_1[0][0]                   \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 100, 110)     1013760     merge_features_frames[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 100, 110)     73040       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 2)       222         bidirectional_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,926,934\n",
      "Trainable params: 5,804,886\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using both as input:\n",
    "model_1_both = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../src/models/train_model.py:320: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 19.0 steps, validate for 1 steps\n",
      "Epoch 1/5\n",
      "19/19 [==============================] - 37s 2s/step - loss: 0.2717 - acc: 0.9184 - val_loss: 0.2316 - val_acc: 0.9137\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 38s 2s/step - loss: 0.1470 - acc: 0.9531 - val_loss: 0.2504 - val_acc: 0.9131\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 28s 1s/step - loss: 0.2005 - acc: 0.9306 - val_loss: 0.3898 - val_acc: 0.8167\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.2186 - acc: 0.9185 - val_loss: 0.2140 - val_acc: 0.9381\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 26s 1s/step - loss: 0.1491 - acc: 0.9442 - val_loss: 0.0699 - val_acc: 0.9818\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model_1_features,\n",
    "                      [X_feat_train_1, X_frames_train_1],\n",
    "                      Y_train_1,\n",
    "                      [X_feat_valid_1, X_frames_valid_1],\n",
    "                      Y_valid_1,\n",
    "                      batch_size=200,\n",
    "                      epochs=5,\n",
    "                      seq_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut to do it all with a script\n",
    "(the last three lines are needed when the script is called from the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 32\n",
      "Valid: 3\n",
      "Test: 2\n",
      "Total: 37\n",
      "/Users/belissen/miniconda3/envs/cslr_models/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 100)          100400    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 2)            202       \n",
      "=================================================================\n",
      "Total params: 352,802\n",
      "Trainable params: 352,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/belissen/Code/cslr_limsi/src/models/train_model.py:320: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 19.0 steps, validate for 1 steps\n",
      "Epoch 1/5\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 0.2282 - acc: 0.9150 - f1K: 0.0369 - precisionK: 0.2722 - recallK: 0.0573\n",
      "Epoch 00001: val_f1K improved from -inf to 0.00094, saving model to recognitionUniqueDictaSign_DS_161113274-best.hdf5\n",
      "19/19 [==============================] - 25s 1s/step - loss: 0.2290 - acc: 0.9157 - f1K: 0.0350 - precisionK: 0.2578 - recallK: 0.0543 - val_loss: 0.3296 - val_acc: 0.8939 - val_f1K: 9.4162e-04 - val_precisionK: 0.0333 - val_recallK: 4.7755e-04\n",
      "Epoch 2/5\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 0.1872 - acc: 0.9335 - f1K: 0.1118 - precisionK: 0.4871 - recallK: 0.0743\n",
      "Epoch 00002: val_f1K improved from 0.00094 to 0.40192, saving model to recognitionUniqueDictaSign_DS_161113274-best.hdf5\n",
      "19/19 [==============================] - 20s 1s/step - loss: 0.1861 - acc: 0.9333 - f1K: 0.1246 - precisionK: 0.5045 - recallK: 0.0822 - val_loss: 0.2134 - val_acc: 0.9065 - val_f1K: 0.4019 - val_precisionK: 0.3323 - val_recallK: 0.5085\n",
      "Epoch 3/5\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 0.1557 - acc: 0.9515 - f1K: 0.0751 - precisionK: 0.5013 - recallK: 0.0462\n",
      "Epoch 00003: val_f1K did not improve from 0.40192\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.1615 - acc: 0.9492 - f1K: 0.0801 - precisionK: 0.4889 - recallK: 0.0503 - val_loss: 0.2551 - val_acc: 0.9104 - val_f1K: 0.0000e+00 - val_precisionK: 0.0000e+00 - val_recallK: 0.0000e+00\n",
      "Epoch 4/5\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 0.1835 - acc: 0.9277 - f1K: 0.1392 - precisionK: 0.6122 - recallK: 0.1008\n",
      "Epoch 00004: val_f1K did not improve from 0.40192\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.1797 - acc: 0.9302 - f1K: 0.1368 - precisionK: 0.5952 - recallK: 0.0985 - val_loss: 0.2437 - val_acc: 0.9131 - val_f1K: 0.0125 - val_precisionK: 0.0797 - val_recallK: 0.0068\n",
      "Epoch 5/5\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 0.1686 - acc: 0.9351 - f1K: 0.1437 - precisionK: 0.5668 - recallK: 0.1012\n",
      "Epoch 00005: val_f1K did not improve from 0.40192\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.1762 - acc: 0.9312 - f1K: 0.1463 - precisionK: 0.5819 - recallK: 0.1016 - val_loss: 0.2577 - val_acc: 0.8729 - val_f1K: 0.3730 - val_precisionK: 0.3306 - val_recallK: 0.4278\n",
      "Results\n",
      "Validation set\n",
      "Framewise accuracy: 0.8894369973190348\n",
      "Framewise P, R, F1: 0.330387314790746, 0.4508691025186236, 0.38133813381338133\n",
      "P*(0,0), R*(0,0), F1*(0,0):0.3647342995169082, 0.6612903225806451, 0.47015492102065615\n",
      "Ip, Ir, Ipr (star): 0.3550084689396291, 0.23371789442490568, 0.2943631816822674\n",
      "margin = 0\n",
      "P, R, F1 (middleUnit): 0.0, 0.0, 0\n",
      "P, R, F1 (marginUnit): 0.3647342995169082, 0.6612903225806451, 0.47015492102065615\n",
      "margin = 12\n",
      "P, R, F1 (middleUnit): 0.37681159420289856, 0.6720430107526881, 0.4828764579154594\n",
      "P, R, F1 (marginUnit): 0.5120772946859904, 0.7903225806451613, 0.6214777146275801\n",
      "margin = 25\n",
      "P, R, F1 (middleUnit): 0.5193236714975845, 0.8279569892473119, 0.6382896689992867\n",
      "P, R, F1 (marginUnit): 0.6183574879227053, 0.8709677419354839, 0.7232395103065815\n",
      "margin = 50\n",
      "P, R, F1 (middleUnit): 0.6714975845410628, 0.9354838709677419, 0.7818076027928627\n",
      "P, R, F1 (marginUnit): 0.714975845410628, 0.9623655913978495, 0.820427060590576\n",
      "Test set\n",
      "/Users/belissen/Code/cslr_limsi/src/models/perf_utils.py:630: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  fStarTp = 2 * 1. / (1. / pStarTp + 1. / rStarTp)\n",
      "/Users/belissen/Code/cslr_limsi/src/models/perf_utils.py:631: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  fStarTr = 2 * 1. / (1. / pStarTr + 1. / rStarTr)\n",
      "Framewise accuracy: 0.9649828178694158\n",
      "Framewise P, R, F1: 0.0, 0.0, 0\n",
      "P*(0,0), R*(0,0), F1*(0,0):0.0, 0.0, 0.0\n",
      "Ip, Ir, Ipr (star): 0.0, 0.0, 0.0\n",
      "margin = 0\n",
      "P, R, F1 (middleUnit): 0.0, 0.0, 0\n",
      "P, R, F1 (marginUnit): 0.0, 0.0, 0\n",
      "margin = 12\n",
      "P, R, F1 (middleUnit): 0.0, 0.0, 0\n",
      "P, R, F1 (marginUnit): 0.0, 0.0, 0\n",
      "margin = 25\n",
      "P, R, F1 (middleUnit): 0.0, 0.0, 0\n",
      "P, R, F1 (marginUnit): 0.0, 0.0, 0\n",
      "margin = 50\n",
      "P, R, F1 (middleUnit): 0.0, 0.0, 0\n",
      "P, R, F1 (marginUnit): 0.0, 0.0, 0\n"
     ]
    }
   ],
   "source": [
    "!python ../src/recognitionUniqueDictaSign.py --outputName DS \\\n",
    "                                             --epochs 5 \\\n",
    "                                             --batchSize 200 \\\n",
    "                                             --videoSplitMode manual \\\n",
    "                                             --signersTrain 0 1 2 3 \\\n",
    "                                             --signersValid 10 \\\n",
    "                                             --signersTest 13 \\\n",
    "                                             --fromNotebook 1 \\\n",
    "                                             --saveGlobalresults ../reports/corpora/DictaSign/recognitionUnique/global/globalUnique.dat \\\n",
    "                                             --savePredictions ../reports/corpora/DictaSign/recognitionUnique/predictions/ \\\n",
    "                                             --saveModels ../models/corpora/DictaSign/recognitionUnique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1bis'></a>\n",
    "#### ex. 1 bis ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_1bis, X_frames_train_1bis], Y_train_1bis =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DSA', 'DSG']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_1bis, X_frames_valid_1bis], Y_valid_1bis =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DSA', 'DSG']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "(1, 371696, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1bis.shape)\n",
    "print(X_frames_train_1bis.shape)\n",
    "print(Y_train_1bis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_1bis_features = get_model(output_names=['DSA-DSG'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2'></a>\n",
    "#### ex. 2 ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_2, X_frames_train_2], Y_train_2 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_2, X_frames_valid_2], Y_valid_2 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2_features = get_model(output_names=['fls'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut to do it all with a script\n",
    "(the last three lines are needed when the script is called from the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/recognitionUniqueDictaSign.py --outputName fls \\\n",
    "                                             --epochs 5 \\\n",
    "                                             --batchSize 200 \\\n",
    "                                             --videoSplitMode manual \\\n",
    "                                             --signersTrain 0 1 2 3 \\\n",
    "                                             --signersValid 10 \\\n",
    "                                             --signersTest 13 \\\n",
    "                                             --fromNotebook 1 \\\n",
    "                                             --saveGlobalresults ../reports/corpora/DictaSign/recognitionUnique/global/globalUnique.dat \\\n",
    "                                             --savePredictions ../reports/corpora/DictaSign/recognitionUnique/predictions/ \\\n",
    "                                             --saveModels ../models/corpora/DictaSign/recognitionUnique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2bis'></a>\n",
    "#### ex. 2 bis ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flsKept=[43015, 43038, 42318, 43357, 43116, 42719]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUI:VAR\n",
      "NON1:VAR\n",
      "COMME/MEME/AUSSI\n",
      "SUPER/BIEN\n",
      "CA VEUT DIRE1:VAR\n",
      "JUSTE1/PRECIS\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "idGloss = {}\n",
    "\n",
    "with open('Dicta-Sign-LSF_ID.csv', newline='') as csvfile:\n",
    "    glossreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    for row in glossreader:\n",
    "        idGloss[row[0]] = row[1]\n",
    "\n",
    "        N_fls = len(flsKept)\n",
    "# that correspond to glosses:\n",
    "for i in flsKept:\n",
    "    print(idGloss[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_2bis, X_frames_train_2bis], Y_train_2bis =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls']],\n",
    "                            nonZero=[flsKept],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_2bis, X_frames_valid_2bis], Y_valid_2bis =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls']],\n",
    "                            nonZero=[flsKept],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "(1, 371696, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_2bis.shape)\n",
    "print(X_frames_train_2bis.shape)\n",
    "print(Y_train_2bis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2bis_features = get_model(output_names=['fls-binary-43015_43038_42318_43357_43116_42719'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut to do it all with a script\n",
    "(the last three lines are needed when the script is called from the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/recognitionUniqueDictaSign.py --outputName fls \\\n",
    "                                             --flsKept 43015 43038 42318 43357 43116 42719 \\\n",
    "                                             --flsBinary 1 \\\n",
    "                                             --epochs 5 \\\n",
    "                                             --batchSize 200 \\\n",
    "                                             --videoSplitMode manual \\\n",
    "                                             --signersTrain 0 1 2 3 \\\n",
    "                                             --signersValid 10 \\\n",
    "                                             --signersTest 13 \\\n",
    "                                             --fromNotebook 1 \\\n",
    "                                             --saveGlobalresults ../reports/corpora/DictaSign/recognitionUnique/global/globalUnique.dat \\\n",
    "                                             --savePredictions ../reports/corpora/DictaSign/recognitionUnique/predictions/ \\\n",
    "                                             --saveModels ../models/corpora/DictaSign/recognitionUnique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2ter'></a>\n",
    "#### ex. 2 ter ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_2ter, X_frames_train_2ter], Y_train_2ter =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['fls'],['fls'],['fls'],['fls'],['fls']],\n",
    "                            nonZero=[[flsKept[0]],[flsKept[1]],[flsKept[2]],[flsKept[3]],[flsKept[4]],[flsKept[5]]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_2ter, X_frames_valid_2ter], Y_valid_2ter =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['fls'],['fls'],['fls'],['fls'],['fls']],\n",
    "                            nonZero=[[flsKept[0]],[flsKept[1]],[flsKept[2]],[flsKept[3]],[flsKept[4]],[flsKept[5]]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "(1, 371696, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_2ter.shape)\n",
    "print(X_frames_train_2ter.shape)\n",
    "print(Y_train_2ter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 100, 7)            777       \n",
      "=================================================================\n",
      "Total params: 438,657\n",
      "Trainable params: 438,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2ter_features = get_model(output_names=['fls-categ-43015_43038_42318_43357_43116_42719'],\n",
    "                    output_classes=[7],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut to do it all with a script\n",
    "(the last three lines are needed when the script is called from the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/recognitionUniqueDictaSign.py --outputName fls \\\n",
    "                                             --flsKept 43015 43038 42318 43357 43116 42719 \\\n",
    "                                             --flsBinary 0 \\\n",
    "                                             --epochs 5 \\\n",
    "                                             --batchSize 200 \\\n",
    "                                             --videoSplitMode manual \\\n",
    "                                             --signersTrain 0 1 2 3 \\\n",
    "                                             --signersValid 10 \\\n",
    "                                             --signersTest 13 \\\n",
    "                                             --fromNotebook 1 \\\n",
    "                                             --saveGlobalresults ../reports/corpora/DictaSign/recognitionUnique/global/globalUnique.dat \\\n",
    "                                             --savePredictions ../reports/corpora/DictaSign/recognitionUnique/predictions/ \\\n",
    "                                             --saveModels ../models/corpora/DictaSign/recognitionUnique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex3'></a>\n",
    "#### ex. 3 ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_3, X_frames_train_3], Y_train_3 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['DS'],['PT'],['FBUOY']],\n",
    "                            nonZero=[[],[],[],[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_3, X_frames_valid_3], Y_valid_3 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['DS'],['PT'],['FBUOY']],\n",
    "                            nonZero=[[],[],[],[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "(1, 371696, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_3.shape)\n",
    "print(X_frames_train_3.shape)\n",
    "print(Y_train_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 100, 5)            555       \n",
      "=================================================================\n",
      "Total params: 438,435\n",
      "Trainable params: 438,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_3_features = get_model(output_names=['fls-DS-PT-FBUOY'],\n",
    "                    output_classes=[5],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second category of examples: `output_form = 'mixed'` (examples 4 to 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex4'></a>\n",
    "#### ex. 4 ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_4, X_frames_train_4], Y_train_4 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)\n",
    "[X_feat_valid_4, X_frames_valid_4], Y_valid_4 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 371696, 420)\n",
      "(371696,)\n",
      "4\n",
      "(1, 371696, 7)\n",
      "(1, 371696, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_4.shape)\n",
    "print(X_frames_train_4.shape)\n",
    "print(len(Y_train_4))\n",
    "print(Y_train_4[0].shape)\n",
    "print(Y_train_4[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 100, 200)     252200      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 100, 110)     112640      conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 100, 110)     73040       bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 100, 7)       777         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 100, 2)       222         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 100, 2)       222         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 100, 2)       222         bidirectional_21[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 439,323\n",
      "Trainable params: 439,323\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4_features = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex5'></a>\n",
    "#### ex. 5 ([back to the list of examples](#list_examples)):\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_5, X_frames_train_5], Y_train_5 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['fls']],\n",
    "                            nonZero=[[],[]],\n",
    "                            binary=[True,True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_features = get_model(output_names=['PT_PRO123', 'fls_all'],\n",
    "                    output_classes=[2,2],\n",
    "                    output_weights=[1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
