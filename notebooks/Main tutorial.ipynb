{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all helper functions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.models.data_utils import *\n",
    "from src.models.model_utils import *\n",
    "from src.models.train_model import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info:\n",
    "\n",
    "### Code structure\n",
    "- Helper functions are stored in src/models\n",
    "- Data is stored in data/processed\n",
    "\n",
    "### Corpora and annotation\n",
    "#### Dicta-Sign\n",
    "Annotations include (see more detail in the ortolang repo):\n",
    "* **fls**: fully-lexical signs, encoded as categorical values (gloss indices)\n",
    "* **PT**:    pointing signs, encoded as binary\n",
    "* **PT_PRO1**, **PT_PRO2**, **PT_PRO3**, **PT_LOC**, **PT_DET**, **PT_LBUOY**, **PT_BUOY**: sub-categories for pointing signs, encoded as binary\n",
    "* **DS**:    depicting signs, encoded as binary\n",
    "* **DSA**, **DSG**, **DSL**, **DSM**, **DSS**, **DST**, **DSX**: sub-categories for depicting signs, encoded as binary\n",
    "* **FBUOY**: fragment buoys, encoded as binary\n",
    "* **N**:     numbering signs, encoded as binary\n",
    "* **FS**:    fingerspelling signs, encoded as binary\n",
    "\n",
    "#### NCSLGR\n",
    "Annotations include (all data is binary):\n",
    "* **lexical_with_ns_not_fs**: lexical signs, including numbering signs but excluding fingerspelling signs\n",
    "* **fingerspelling**, **fingerspelled_loan_signs**: fingerspelling signs, finglerspelling loan signs\n",
    "* **IX_1p**, **IX_2p**, **IX_3p**, **IX_loc**: sub-categories for pointing signs\n",
    "* **POSS**, **SELF**: possessive pronouns\n",
    "* **DCL**, **LCL**, **SCL**, **BCL**, **ICL**, **BPCL**, **PCL**: sub-categories for classifier signs (i.e. depicting signs)\n",
    "* **gesture**: culturally shared gestures\n",
    "* **part_indef**\n",
    "* **other**\n",
    "\n",
    "### Type of input features\n",
    "Originally, this code was designed around preprocessed features for each frame. Possible features types are : \n",
    "- **2Draw**\n",
    "- **2Draw_HS**\n",
    "- **2Draw_HS_noOP**\n",
    "- **2Draw_noHands**\n",
    "- **2Dfeatures**\n",
    "- **2Dfeatures_HS**\n",
    "- **2Dfeatures_HS_noOP**\n",
    "- **2Dfeatures_noHands**\n",
    "- **3Draw**\n",
    "- **3Draw_HS**\n",
    "- **3Draw_HS_noOP**\n",
    "- **3Draw_noHands**\n",
    "- **3Dfeatures**\n",
    "- **3Dfeatures_HS**\n",
    "- **3Dfeatures_HS_noOP**\n",
    "- **3Dfeatures_noHands**\n",
    "\n",
    "which correspond to 2D or 3D data, raw OpenPose or preprocessed body and face data, including or excluding hand shape estimates, including or excluding OpenPose hand data.\n",
    "\n",
    "The main data function `get_data_concatenated` requires a features dictionary, which can be obtained with the `getFeaturesDict` function.\n",
    "\n",
    "Recently, we also added direct image input to the model, but it has not been tested thoroughly.\n",
    "\n",
    "### Model outputs\n",
    "The model can be trained for the recognition of:\n",
    "- one or several (N) mixed linguistic descriptors in parallel, possibly simultaneously true. Each descriptor includes a 'garbage/other' class.\n",
    "    - **ex. 1** (N = 4) : Y1 = [Y1_1 : lexical signs (categorical with 4 different signs), Y1_2 : pointing signs, Y1_3 : depicting signs, Y1_4 : fragment buoys]\n",
    "    - **ex. 2** (N = 2) : Y2 = [Y2_1 : pointing signs to PRO1/2/3, Y2_2 : lexical signs (all)]\n",
    "    - **ex. 3** (N = 2) : Y3 = [Y3_1 : pointing signs to PRO1/2/3, Y3_2 : depicting signs A/G]\n",
    "    - **ex. 4** (N = 1) : Y4 = [Y4_1 : depicting signs]\n",
    "- sign types, i.e. the most probable sign type for each frame. In this case, lexical signs are seen as binary.\n",
    "    - **ex. 5**: Y5 = [other, lexical signs,  pointing signs, depicting signs, fragment buoys]. \n",
    "    - **ex. 6**: Y6 = [other, depicting signs] : this should give the same results as ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help on a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_raw_annotation_from_file in module src.models.data_utils:\n",
      "\n",
      "get_raw_annotation_from_file(corpus, from_notebook=False)\n",
      "    Gets raw annotation from data file\n",
      "    \n",
      "    Inputs:\n",
      "        corpus: 'DictaSign' or 'NCSLGR'\n",
      "        from_notebook: True if used in Jupyter notebook\n",
      "    \n",
      "    Outputs:\n",
      "        Annotation data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help(function_name)\n",
    "# For instance:\n",
    "\n",
    "help(get_raw_annotation_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for data handling (`data_utils.py`): `get_data_concatenated`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to extract data in usable format for training.\n",
    "\n",
    "`get_data_concatenated` returns [X_features, X_frames], Y, idx_trueData or [X_features, X_frames], Y (depending on return_idx_trueData)\n",
    "\n",
    "#### Outputs\n",
    "- **X_features** is a numpy array of size [1, total_time_steps, features_number] containing all retained preprocessed features for all retained frames\n",
    "- **X_frames** is simply a list of paths for all retained frames (frames cannot be stored in memory directly, and will have to be read during training thanks to frames paths)\n",
    "- **Y** is the annotation data (i.e. ground truth data) in the desired format\n",
    "\n",
    "#### Inputs\n",
    "- **corpus** (string)\n",
    "- **output_form**:\n",
    "    - 'mixed' if different and separated Outputs\n",
    "    - 'sign_types' if annotation is only a binary matrix of sign types\n",
    "- **types**: a list of lists of original names that are used to compose final outputs\n",
    "- **nonZero**: a list of lists of nonzero values to consider. If 4 outputs with all nonZero values should be considered, nonZero=[[],[],[],[]]\n",
    "- **binary**: only considered when output_form=mixed. It's a list (True/False) indicating whether the values should be categorical or binary\n",
    "- **features_dict**: a dictionary indication which features to keep ; e.g.: {'features_HS':np.arange(0, 420), 'features_HS_norm':np.array([]), 'raw':np.array([]), 'raw_norm':np.array([])}\n",
    "- **preloaded_features**: if features are already loaded, in the format of a list (features for each video)\n",
    "- **provided_annotation**: raw annotation data (not needed)\n",
    "- **video_indices**: numpy array for a list of videos\n",
    "- **separation**: in order to separate consecutive videos\n",
    "- **from_notebook**: if notebook script, data is in parent folder\n",
    "- **return_idx_trueData**: if True, returns a binary vector with 0 where separations are\n",
    "- **features_type**: 'features', 'frames', 'both'            \n",
    "- **frames_path_before_video**: video frames are supposed to be in folders, like '/localHD/DictaSign/convert/img/DictaSign_lsf_S7_T2_A10',\n",
    "- **empty_image_path**: path of a white frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for model handling (`model_utils.py`): `get_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to obtain the Keras model.\n",
    "\n",
    "`get_model` returns a Keras model\n",
    "\n",
    "#### Outputs\n",
    "- a Keras model\n",
    "\n",
    "#### Inputs\n",
    "- **output_names**: list of outputs (strings)\n",
    "- **output_classes**: list of number of classes of each output type\n",
    "- **output_weights**: list of weights for each_output\n",
    "- **conv** (bool): if True, applies convolution on input\n",
    "- **conv_filt**: number of convolution filters\n",
    "- **conv_ker**: size of convolution kernel\n",
    "- **conv_strides**: size of convolution strides\n",
    "- **rnn_number**: number of recurrent layers\n",
    "- **rnn_type**: type of recurrent layers (string)\n",
    "- **rnn_hidden_units**: number of hidden units\n",
    "- **dropout**: how much dropout (0 to 1)\n",
    "- **att_in_rnn**: if True, applies attention layer before recurrent layers\n",
    "- **att_in_rnn_single**: single (shared) attention layer or not\n",
    "- **att_in_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **att_out_rnn**: if True, applies attention layer after recurrent layers\n",
    "- **att_out_rnn_single**: single (shared) attention layer or not\n",
    "- **att_out_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **rnn_return_sequences**: if False, only last timestep of recurrent layers is returned\n",
    "- **classif_local** (bool): whether classification is for each timestep (local) of globally for the sequence\n",
    "- **mlp_layers_number**: number of additional dense layers\n",
    "- **mlp_layers_size**: size of additional dense layers\n",
    "- **optimizer**: gradient optimizer type (string)\n",
    "- **learning_rate**: learning rate (float)\n",
    "- **time_steps**: length of sequences (int)\n",
    "- **features_number**: number of features (int)\n",
    "- **features_type**: 'features' (1D vector of features), 'frames' (for a CNN processing) or 'both'\n",
    "- **img_height** and **img_width**: size of CNN input\n",
    "- **cnnType**: 'resnet', 'vgg' or 'mobilenet'\n",
    "- **cnnFirstTrainedLayer**: index of first trainable layer in CNN (int)\n",
    "- **cnnReduceDim**: if greater than 0, size of CNN flattened output is reduced to cnnReduceDim\n",
    "- **print_summary** (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut: script to recognize a unique output on DictaSign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use `python src/recognitionUniqueDictaSignFromScript.py`\n",
    "\n",
    "Provided help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: recognitionUniqueDictaSignFromScript.py [-h] [--outputName OUTPUTNAME]\r\n",
      "                                               [--flsBinary {0,1}]\r\n",
      "                                               [--flsKeep [FLSKEEP [FLSKEEP ...]]]\r\n",
      "                                               [--comment COMMENT]\r\n",
      "                                               [--videoSplitMode {manual,auto}]\r\n",
      "                                               [--fractionValid FRACTIONVALID]\r\n",
      "                                               [--fractionTest FRACTIONTEST]\r\n",
      "                                               [--signerIndependent {0,1}]\r\n",
      "                                               [--taskIndependent {0,1}]\r\n",
      "                                               [--excludeTask9 {0,1}]\r\n",
      "                                               [--tasksTrain [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                               [--tasksValid [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                               [--tasksTest [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]]\r\n",
      "                                               [--signersTrain [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                               [--signersValid [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                               [--signersTest [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]]\r\n",
      "                                               [--idxTrainBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                               [--idxValidBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                               [--idxTestBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]]\r\n",
      "                                               [--randSeed RANDSEED]\r\n",
      "                                               [--weightCorrection WEIGHTCORRECTION]\r\n",
      "                                               [--inputType {2Draw,2Draw_HS,2Draw_HS_noOP,2Draw_noHands,2Dfeatures,2Dfeatures_HS,2Dfeatures_HS_noOP,2Dfeatures_noHands,3Draw,3Draw_HS,3Draw_HS_noOP,3Draw_noHands,3Dfeatures,3Dfeatures_HS,3Dfeatures_HS_noOP,3Dfeatures_noHands,none}]\r\n",
      "                                               [--inputNormed {0,1}]\r\n",
      "                                               [--inputFeaturesFrames {features,frames,both}]\r\n",
      "                                               [--imgWidth {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}]\r\n",
      "                                               [--imgHeight {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}]\r\n",
      "                                               [--cnnType {resnet,vgg,mobilenet}]\r\n",
      "                                               [--cnnFirstTrainedLayer CNNFIRSTTRAINEDLAYER]\r\n",
      "                                               [--cnnReduceDim CNNREDUCEDIM]\r\n",
      "                                               [--seqLength SEQLENGTH]\r\n",
      "                                               [--batchSize BATCHSIZE]\r\n",
      "                                               [--epochs EPOCHS]\r\n",
      "                                               [--separation SEPARATION]\r\n",
      "                                               [--dropout DROPOUT]\r\n",
      "                                               [--rnnNumber RNNNUMBER]\r\n",
      "                                               [--rnnHiddenUnits RNNHIDDENUNITS]\r\n",
      "                                               [--mlpLayersNumber MLPLAYERSNUMBER]\r\n",
      "                                               [--convolution {0,1}]\r\n",
      "                                               [--convFilt CONVFILT]\r\n",
      "                                               [--convFiltSize CONVFILTSIZE]\r\n",
      "                                               [--learningRate LEARNINGRATE]\r\n",
      "                                               [--optimizer {rms,ada,sgd}]\r\n",
      "                                               [--earlyStopping {0,1}]\r\n",
      "                                               [--redLrOnPlat {0,1}]\r\n",
      "                                               [--redLrMonitor REDLRMONITOR]\r\n",
      "                                               [--redLrMonitorMode {min,max}]\r\n",
      "                                               [--redLrPatience REDLRPATIENCE]\r\n",
      "                                               [--redLrFactor REDLRFACTOR]\r\n",
      "                                               [--saveModel {no,best,all}]\r\n",
      "                                               [--saveBestMonitor SAVEBESTMONITOR]\r\n",
      "                                               [--saveBestMonMode {min,max}]\r\n",
      "                                               [--saveGlobalresults SAVEGLOBALRESULTS]\r\n",
      "                                               [--savePredictions SAVEPREDICTIONS]\r\n",
      "                                               [--stepWolf {rms,ada,sgd}]\r\n",
      "\r\n",
      "Trains a Keras-TF model for the recognition of a unique type of annotation, on\r\n",
      "the DictaSign-LSF-v2 corpus\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --outputName OUTPUTNAME\r\n",
      "                        The output type that the model is trained to recognize\r\n",
      "  --flsBinary {0,1}     If the output is FLS, if seen as binary\r\n",
      "  --flsKeep [FLSKEEP [FLSKEEP ...]]\r\n",
      "                        If the output is FLS, list of FLS indices to consider\r\n",
      "  --comment COMMENT     A comment to describe this run\r\n",
      "  --videoSplitMode {manual,auto}\r\n",
      "                        Split mode for videos (auto or manually specified)\r\n",
      "  --fractionValid FRACTIONVALID\r\n",
      "                        Fraction of valid data wrt total (if auto split mode)\r\n",
      "  --fractionTest FRACTIONTEST\r\n",
      "                        Fraction of test data wrt total (if auto split mode)\r\n",
      "  --signerIndependent {0,1}\r\n",
      "                        Signer independent train/valid/test random shuffle\r\n",
      "  --taskIndependent {0,1}\r\n",
      "                        Task independent train/valid/test random shuffle\r\n",
      "  --excludeTask9 {0,1}  Whether to exclude task 9\r\n",
      "  --tasksTrain [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Training task indices\r\n",
      "  --tasksValid [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Validation task indices\r\n",
      "  --tasksTest [{1,2,3,4,5,6,7,8,9} [{1,2,3,4,5,6,7,8,9} ...]]\r\n",
      "                        Test task indices\r\n",
      "  --signersTrain [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Training signer indices\r\n",
      "  --signersValid [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Validation signer indices\r\n",
      "  --signersTest [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} ...]]\r\n",
      "                        Test signer indices\r\n",
      "  --idxTrainBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --idxValidBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --idxTestBypass [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} [{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93} ...]]\r\n",
      "                        If you really want to set video indices directly\r\n",
      "  --randSeed RANDSEED   Random seed (numpy)\r\n",
      "  --weightCorrection WEIGHTCORRECTION\r\n",
      "                        Correction for data imbalance (from 0 (no correction)\r\n",
      "                        to 1)\r\n",
      "  --inputType {2Draw,2Draw_HS,2Draw_HS_noOP,2Draw_noHands,2Dfeatures,2Dfeatures_HS,2Dfeatures_HS_noOP,2Dfeatures_noHands,3Draw,3Draw_HS,3Draw_HS_noOP,3Draw_noHands,3Dfeatures,3Dfeatures_HS,3Dfeatures_HS_noOP,3Dfeatures_noHands,none}\r\n",
      "                        Type of features\r\n",
      "  --inputNormed {0,1}   If features are normed\r\n",
      "  --inputFeaturesFrames {features,frames,both}\r\n",
      "                        Features type\r\n",
      "  --imgWidth {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}\r\n",
      "                        CNN width\r\n",
      "  --imgHeight {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999}\r\n",
      "                        CNN height\r\n",
      "  --cnnType {resnet,vgg,mobilenet}\r\n",
      "                        CNN type\r\n",
      "  --cnnFirstTrainedLayer CNNFIRSTTRAINEDLAYER\r\n",
      "                        Index of first trained layer in CNN\r\n",
      "  --cnnReduceDim CNNREDUCEDIM\r\n",
      "                        If greater than 0, the reduced dimension of CNN output\r\n",
      "                        vector\r\n",
      "  --seqLength SEQLENGTH\r\n",
      "                        Length of sequences\r\n",
      "  --batchSize BATCHSIZE\r\n",
      "                        Batch size\r\n",
      "  --epochs EPOCHS       Number of epochs\r\n",
      "  --separation SEPARATION\r\n",
      "                        Separation between videos\r\n",
      "  --dropout DROPOUT     Dropout (0 to 1)\r\n",
      "  --rnnNumber RNNNUMBER\r\n",
      "                        Number of RNN layers\r\n",
      "  --rnnHiddenUnits RNNHIDDENUNITS\r\n",
      "                        Number of hidden units in RNN\r\n",
      "  --mlpLayersNumber MLPLAYERSNUMBER\r\n",
      "                        Number MLP layers after RNN\r\n",
      "  --convolution {0,1}   Whether to use a conv. layer\r\n",
      "  --convFilt CONVFILT   Number of convolution kernels\r\n",
      "  --convFiltSize CONVFILTSIZE\r\n",
      "                        Size of convolution kernels\r\n",
      "  --learningRate LEARNINGRATE\r\n",
      "                        Learning rate\r\n",
      "  --optimizer {rms,ada,sgd}\r\n",
      "                        Training optimizer\r\n",
      "  --earlyStopping {0,1}\r\n",
      "                        Early stopping\r\n",
      "  --redLrOnPlat {0,1}   Reduce l_rate on plateau\r\n",
      "  --redLrMonitor REDLRMONITOR\r\n",
      "                        Metric for l_rate reduction\r\n",
      "  --redLrMonitorMode {min,max}\r\n",
      "                        Mode for l_rate reduction\r\n",
      "  --redLrPatience REDLRPATIENCE\r\n",
      "                        Patience before l_rate reduc\r\n",
      "  --redLrFactor REDLRFACTOR\r\n",
      "                        Factor for each l_rate reduc\r\n",
      "  --saveModel {no,best,all}\r\n",
      "                        Whether to save only best model, or all, or none\r\n",
      "  --saveBestMonitor SAVEBESTMONITOR\r\n",
      "                        What metric to decide best model\r\n",
      "  --saveBestMonMode {min,max}\r\n",
      "                        Mode to define best\r\n",
      "  --saveGlobalresults SAVEGLOBALRESULTS\r\n",
      "                        Where to save global results\r\n",
      "  --savePredictions SAVEPREDICTIONS\r\n",
      "                        Where to save predictions\r\n",
      "  --stepWolf {rms,ada,sgd}\r\n",
      "                        Step between Wolf metric eval points\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/recognitionUniqueDictaSignFromScript.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `python src/recognitionUniqueDictaSignFromScript.py --outputName DS` to select DS as output\n",
    "* `python src/recognitionUniqueDictaSignFromScript.py --outputName fls --flsBinary 1` to select binary FLS as output\n",
    "* `python src/recognitionUniqueDictaSignFromScript.py --outputName fls --flsBinary 0 --flsKeep 41891 43413 43422 42992` to select categorical FLS as output, with 4 signs\n",
    "* `python src/recognitionUniqueDictaSignFromScript.py --outputName DS --inputType 2Draw` to select 2D raw input preprocessed features\n",
    "* `python src/recognitionUniqueDictaSignFromScript.py --outputName DS --inputFeaturesFrames frames` to select only frames as input (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building data and model together, manually\n",
    "\n",
    "In these examples we analyze the different types of output form Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 66\n",
      "Valid: 10\n",
      "Test: 18\n",
      "Total: 94\n"
     ]
    }
   ],
   "source": [
    "# let us split train/valid/test videos\n",
    "# In this case we split by signers in a manual fashion\n",
    "\n",
    "idxTrain, idxValid, idxTest = getVideoIndicesSplitDictaSign(tasksTrain=[],\n",
    "                                                            tasksValid=[],\n",
    "                                                            tasksTest=[],\n",
    "                                                            signersTrain=[0,1,2,3,4,5,6,7,8,9],\n",
    "                                                            signersValid=[10,11,12],\n",
    "                                                            signersTest=[13,14,15],\n",
    "                                                            excludeTask9=False,\n",
    "                                                            videoSplitMode='manual',\n",
    "                                                            checkSplits=True,\n",
    "                                                            checkSets=True,\n",
    "                                                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features_HS': array([], dtype=float64), 'features_HS_norm': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419]), 'raw': array([], dtype=float64), 'raw_norm': array([], dtype=float64), '2Dfeatures': array([], dtype=float64), '2Dfeatures_norm': array([], dtype=float64)}\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "# Getting a dictionary for desired preprocessed features\n",
    "# In this case we ask for normalized 3Dfeatures_HS (this correspond to a total number of 420 features):\n",
    "\n",
    "features_dict, features_number = getFeaturesDict(inputType='3Dfeatures_HS', inputNormed=True)\n",
    "\n",
    "print(features_dict)\n",
    "print(features_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First category of examples: `output_form = 'mixed'` (examples 1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "idGloss = {}\n",
    "\n",
    "with open('Dicta-Sign-LSF_ID.csv', newline='') as csvfile:\n",
    "    glossreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    for row in glossreader:\n",
    "        idGloss[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARIS3 (TOUR EIFFEL):NS\n",
      "RESTAURANT\n",
      "QUALITE-DE GRANDE /EXCELLENT\n",
      "VISITER1:VAR\n"
     ]
    }
   ],
   "source": [
    "# we only consider 4 lexical signs, indices 41891,43413,43422,42992\n",
    "\n",
    "flsKept = [41891,43413,43422,42992]\n",
    "N_fls = len(flsKept)\n",
    "\n",
    "# that correspond to glosses:\n",
    "\n",
    "for i in flsKept:\n",
    "    print(idGloss[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_1, X_frames_train_1], Y_train_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1.shape)\n",
    "print(X_frames_train_1.shape)\n",
    "print(len(Y_train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, X_feat is a big matrix storing all 420 preprocessed features for each frame. We can print the first 15 features for frame number 192:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.56896496e-01 8.08442011e-02 6.51594326e-02 4.68903668e-02\n",
      " 7.92527862e-05 2.17038882e-03 3.29143912e-01 1.75906322e-03\n",
      " 2.52671860e-04 2.70878343e-04 4.83787793e-04 4.01530191e-02\n",
      " 3.07233859e-04 4.62686792e-02 4.28746128e-03]\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1[0,192,0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_frames stores paths for all frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localHD/DictaSign/convert/img/DictaSign_lsf_S4_T8_B14_front/00193.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_frames_train_1[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train stores the 4 linguistic descriptors annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 5)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0].shape)\n",
    "print(Y_train_1[1].shape)\n",
    "print(Y_train_1[2].shape)\n",
    "print(Y_train_1[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1916\n"
     ]
    }
   ],
   "source": [
    "# First depicting frame:\n",
    "i_DS_one = np.where(Y_train_1[1][0,:,1]==1)[0][0]\n",
    "print(i_DS_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0][0,i_DS_one,:])\n",
    "print(Y_train_1[1][0,i_DS_one,:])\n",
    "print(Y_train_1[2][0,i_DS_one,:])\n",
    "print(Y_train_1[3][0,i_DS_one,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame 192 is annotated as non-lexical, depicting, non-pointing, non-fbuoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 100, 200)     252200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 110)     112640      conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 110)     73040       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 5)       555         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 439,101\n",
      "Trainable params: 439,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# training model can be built to take preprocessed features as input, frames as input or both\n",
    "model_1_features = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 100, 2048)    23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 110)     925760      time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 110)     73040       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 5)       555         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,587,733\n",
      "Trainable params: 5,465,685\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_frames = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 200)     252200      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 100, 2048)    23587712    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_frames (Concaten (None, 100, 2248)    0           conv1d_1[0][0]                   \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100, 110)     1013760     merge_features_frames[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 100, 110)     73040       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 100, 5)       555         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,927,933\n",
      "Trainable params: 5,805,885\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_both = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category, \n",
    "# and a second category corresponds to all lexical signs (binary)\n",
    "[X_feat_train_2, X_frames_train_2], Y_train_2 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['fls']],\n",
    "                            nonZero=[[],[]],\n",
    "                            binary=[True,True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_2.shape)\n",
    "print(X_frames_train_2.shape)\n",
    "print(len(Y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_2[0].shape)\n",
    "print(Y_train_2[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 200)     252200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 100, 110)     112640      conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 100, 110)     73040       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 100, 2)       222         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 100, 2)       222         bidirectional_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 438,324\n",
      "Trainable params: 438,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2_features = get_model(output_names=['pointing-signs-pro123', 'lexical'],\n",
    "                    output_classes=[2,2],\n",
    "                    output_weights=[1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category\n",
    "# and data from 2 channels (DSA, DSG) are assembled to form a second category\n",
    "[X_feat_train_3, X_frames_train_3], Y_train_3 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['DSA', 'DSG']],\n",
    "                            nonZero=[[],[]],\n",
    "                            binary=[True,True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_3.shape)\n",
    "print(X_frames_train_3.shape)\n",
    "print(len(Y_train_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_3[0].shape)\n",
    "print(Y_train_3[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 100, 200)     252200      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 100, 110)     112640      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 100, 110)     73040       bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 438,324\n",
      "Trainable params: 438,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_3_features = get_model(output_names=['pointing-signs-pro123', 'depicting-signs-AG'],\n",
    "                    output_classes=[2, 2],\n",
    "                    output_weights=[1, 1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, only depicting signs as output\n",
    "[X_feat_train_4, X_frames_train_4], Y_train_4 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_4.shape)\n",
    "print(X_frames_train_4.shape)\n",
    "print(len(Y_train_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "[[623477.  42135.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_4[0].shape)\n",
    "print(np.sum(Y_train_4[0],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_4_features = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second category of examples: `output_form = 'sign_types'` (examples 5 and 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 5 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_5, X_frames_train_5], Y_train_5 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['DS'],['PT'],['FBUOY']],\n",
    "                            nonZero=[[],[],[],[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "(1, 665612, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_5.shape)\n",
    "print(X_frames_train_5.shape)\n",
    "print(Y_train_5.shape) # when output_form='sign_types', Y is not a list, but a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[488403. 126592.  42135.  15146.   9260.]]\n"
     ]
    }
   ],
   "source": [
    "#print(Y_train_5[0,0:200,:])\n",
    "print(np.sum(Y_train_5,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 100, 5)            555       \n",
      "=================================================================\n",
      "Total params: 438,435\n",
      "Trainable params: 438,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_5_features = get_model(output_names=['fls-DS-PT-FBUOY'],\n",
    "                    output_classes=[5],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 6 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_6, X_frames_train_6], Y_train_6 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_6.shape)\n",
    "print(X_frames_train_6.shape)\n",
    "print(Y_train_6.shape) # when output_form='sign_types', Y is not a list, but a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[623477.  42135.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_6[0,1000:1020,:])\n",
    "print(np.sum(Y_train_6,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_6_features = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "Now we look at how to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs=10\n",
    "seq_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_valid_1, X_frames_valid_1], Y_valid_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 67.0 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 41s 614ms/step - loss: 0.4873 - time_distributed_loss: 0.0378 - time_distributed_1_loss: 0.2391 - time_distributed_2_loss: 0.1289 - time_distributed_3_loss: 0.0816 - time_distributed_acc: 0.9846 - time_distributed_1_acc: 0.9148 - time_distributed_2_acc: 0.9678 - time_distributed_3_acc: 0.9799 - val_loss: 0.0153 - val_time_distributed_loss: 2.9153e-04 - val_time_distributed_1_loss: 0.0058 - val_time_distributed_2_loss: 0.0081 - val_time_distributed_3_loss: 0.0011 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 1.0000 - val_time_distributed_2_acc: 0.9987 - val_time_distributed_3_acc: 1.0000\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 35s 516ms/step - loss: 0.3635 - time_distributed_loss: 0.0075 - time_distributed_1_loss: 0.1865 - time_distributed_2_loss: 0.0963 - time_distributed_3_loss: 0.0732 - time_distributed_acc: 0.9991 - time_distributed_1_acc: 0.9343 - time_distributed_2_acc: 0.9772 - time_distributed_3_acc: 0.9830 - val_loss: 0.6280 - val_time_distributed_loss: 1.0964e-04 - val_time_distributed_1_loss: 0.2852 - val_time_distributed_2_loss: 0.0693 - val_time_distributed_3_loss: 0.2734 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9274 - val_time_distributed_2_acc: 0.9860 - val_time_distributed_3_acc: 0.9564\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 38s 567ms/step - loss: 0.3096 - time_distributed_loss: 0.0047 - time_distributed_1_loss: 0.1663 - time_distributed_2_loss: 0.0872 - time_distributed_3_loss: 0.0514 - time_distributed_acc: 0.9994 - time_distributed_1_acc: 0.9381 - time_distributed_2_acc: 0.9785 - time_distributed_3_acc: 0.9884 - val_loss: 0.5256 - val_time_distributed_loss: 5.4706e-04 - val_time_distributed_1_loss: 0.2984 - val_time_distributed_2_loss: 0.1420 - val_time_distributed_3_loss: 0.0847 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.8809 - val_time_distributed_2_acc: 0.9650 - val_time_distributed_3_acc: 0.9852\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 41s 606ms/step - loss: 0.3050 - time_distributed_loss: 0.0103 - time_distributed_1_loss: 0.1583 - time_distributed_2_loss: 0.0793 - time_distributed_3_loss: 0.0570 - time_distributed_acc: 0.9987 - time_distributed_1_acc: 0.9382 - time_distributed_2_acc: 0.9794 - time_distributed_3_acc: 0.9854 - val_loss: 0.3719 - val_time_distributed_loss: 0.0122 - val_time_distributed_1_loss: 0.1948 - val_time_distributed_2_loss: 0.1000 - val_time_distributed_3_loss: 0.0649 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.9251 - val_time_distributed_2_acc: 0.9750 - val_time_distributed_3_acc: 0.9900\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.3225 - time_distributed_loss: 0.0080 - time_distributed_1_loss: 0.1597 - time_distributed_2_loss: 0.0908 - time_distributed_3_loss: 0.0640 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9356 - time_distributed_2_acc: 0.9741 - time_distributed_3_acc: 0.9807 - val_loss: 0.2869 - val_time_distributed_loss: 9.6995e-04 - val_time_distributed_1_loss: 0.1449 - val_time_distributed_2_loss: 0.0801 - val_time_distributed_3_loss: 0.0609 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9370 - val_time_distributed_2_acc: 0.9765 - val_time_distributed_3_acc: 0.9741\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 38s 563ms/step - loss: 0.2328 - time_distributed_loss: 0.0077 - time_distributed_1_loss: 0.1178 - time_distributed_2_loss: 0.0672 - time_distributed_3_loss: 0.0401 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9557 - time_distributed_2_acc: 0.9806 - time_distributed_3_acc: 0.9888 - val_loss: 0.3588 - val_time_distributed_loss: 4.6662e-04 - val_time_distributed_1_loss: 0.1751 - val_time_distributed_2_loss: 0.0942 - val_time_distributed_3_loss: 0.0891 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9162 - val_time_distributed_2_acc: 0.9720 - val_time_distributed_3_acc: 0.9652\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 36s 542ms/step - loss: 0.2762 - time_distributed_loss: 0.0067 - time_distributed_1_loss: 0.1452 - time_distributed_2_loss: 0.0812 - time_distributed_3_loss: 0.0431 - time_distributed_acc: 0.9991 - time_distributed_1_acc: 0.9425 - time_distributed_2_acc: 0.9757 - time_distributed_3_acc: 0.9885 - val_loss: 0.4969 - val_time_distributed_loss: 0.0121 - val_time_distributed_1_loss: 0.2822 - val_time_distributed_2_loss: 0.1370 - val_time_distributed_3_loss: 0.0656 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.8761 - val_time_distributed_2_acc: 0.9671 - val_time_distributed_3_acc: 0.9896\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 37s 548ms/step - loss: 0.2753 - time_distributed_loss: 0.0079 - time_distributed_1_loss: 0.1408 - time_distributed_2_loss: 0.0827 - time_distributed_3_loss: 0.0438 - time_distributed_acc: 0.9989 - time_distributed_1_acc: 0.9463 - time_distributed_2_acc: 0.9740 - time_distributed_3_acc: 0.9879 - val_loss: 0.4673 - val_time_distributed_loss: 9.3776e-04 - val_time_distributed_1_loss: 0.2463 - val_time_distributed_2_loss: 0.1402 - val_time_distributed_3_loss: 0.0799 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9081 - val_time_distributed_2_acc: 0.9609 - val_time_distributed_3_acc: 0.9805\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 36s 534ms/step - loss: 0.2331 - time_distributed_loss: 0.0069 - time_distributed_1_loss: 0.1176 - time_distributed_2_loss: 0.0667 - time_distributed_3_loss: 0.0418 - time_distributed_acc: 0.9989 - time_distributed_1_acc: 0.9557 - time_distributed_2_acc: 0.9800 - time_distributed_3_acc: 0.9867 - val_loss: 0.4975 - val_time_distributed_loss: 9.4410e-04 - val_time_distributed_1_loss: 0.2721 - val_time_distributed_2_loss: 0.1187 - val_time_distributed_3_loss: 0.1057 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.8927 - val_time_distributed_2_acc: 0.9681 - val_time_distributed_3_acc: 0.9705\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.2248 - time_distributed_loss: 0.0044 - time_distributed_1_loss: 0.1232 - time_distributed_2_loss: 0.0610 - time_distributed_3_loss: 0.0361 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9533 - time_distributed_2_acc: 0.9805 - time_distributed_3_acc: 0.9896 - val_loss: 0.3449 - val_time_distributed_loss: 0.0149 - val_time_distributed_1_loss: 0.2065 - val_time_distributed_2_loss: 0.0729 - val_time_distributed_3_loss: 0.0505 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.9277 - val_time_distributed_2_acc: 0.9821 - val_time_distributed_3_acc: 0.9923\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model_1_features,\n",
    "                      [X_feat_train_1, X_frames_train_1],\n",
    "                      Y_train_1,\n",
    "                      [X_feat_valid_1, X_frames_valid_1],\n",
    "                      Y_valid_1,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4873052234969922,\n",
       "  0.36351746926779177,\n",
       "  0.3095500692169168,\n",
       "  0.3050421726014187,\n",
       "  0.3225172839836398,\n",
       "  0.23277043945976159,\n",
       "  0.27617699491666325,\n",
       "  0.2752775760617719,\n",
       "  0.23305533198055936,\n",
       "  0.22476121366246424],\n",
       " 'time_distributed_loss': [0.037806444,\n",
       "  0.0075106374,\n",
       "  0.0046897423,\n",
       "  0.010325068,\n",
       "  0.0080221575,\n",
       "  0.0076571107,\n",
       "  0.0066989427,\n",
       "  0.007943476,\n",
       "  0.006906639,\n",
       "  0.0044149715],\n",
       " 'time_distributed_1_loss': [0.23907705,\n",
       "  0.18651846,\n",
       "  0.16627303,\n",
       "  0.15832663,\n",
       "  0.15969767,\n",
       "  0.11779289,\n",
       "  0.14515437,\n",
       "  0.14083445,\n",
       "  0.11762877,\n",
       "  0.12321183],\n",
       " 'time_distributed_2_loss': [0.12885219,\n",
       "  0.09632742,\n",
       "  0.08722829,\n",
       "  0.07934722,\n",
       "  0.09076402,\n",
       "  0.06719639,\n",
       "  0.081218965,\n",
       "  0.08266508,\n",
       "  0.06670073,\n",
       "  0.061022077],\n",
       " 'time_distributed_3_loss': [0.081569545,\n",
       "  0.07316094,\n",
       "  0.05135904,\n",
       "  0.057043206,\n",
       "  0.06403344,\n",
       "  0.040124036,\n",
       "  0.0431047,\n",
       "  0.04383459,\n",
       "  0.041819192,\n",
       "  0.036112316],\n",
       " 'time_distributed_acc': [0.9846254,\n",
       "  0.99911195,\n",
       "  0.9994433,\n",
       "  0.9987075,\n",
       "  0.99895376,\n",
       "  0.99903136,\n",
       "  0.9990791,\n",
       "  0.99894476,\n",
       "  0.9989239,\n",
       "  0.9990403],\n",
       " 'time_distributed_1_acc': [0.91484773,\n",
       "  0.9342851,\n",
       "  0.9380612,\n",
       "  0.9382,\n",
       "  0.9355597,\n",
       "  0.9557433,\n",
       "  0.9424582,\n",
       "  0.94625074,\n",
       "  0.9556776,\n",
       "  0.95326716],\n",
       " 'time_distributed_2_acc': [0.9678045,\n",
       "  0.9771687,\n",
       "  0.9785299,\n",
       "  0.979406,\n",
       "  0.9740806,\n",
       "  0.98060894,\n",
       "  0.9756522,\n",
       "  0.9740224,\n",
       "  0.98003435,\n",
       "  0.98050594],\n",
       " 'time_distributed_3_acc': [0.9799134,\n",
       "  0.98297316,\n",
       "  0.98841345,\n",
       "  0.9854448,\n",
       "  0.9806746,\n",
       "  0.9888418,\n",
       "  0.98851943,\n",
       "  0.9879418,\n",
       "  0.9867119,\n",
       "  0.9895582],\n",
       " 'val_loss': [0.01528966799378395,\n",
       "  0.6279760599136353,\n",
       "  0.525587797164917,\n",
       "  0.3719367980957031,\n",
       "  0.2869053781032562,\n",
       "  0.35882410407066345,\n",
       "  0.49687668681144714,\n",
       "  0.46732157468795776,\n",
       "  0.4974942207336426,\n",
       "  0.3448622524738312],\n",
       " 'val_time_distributed_loss': [0.00029152574,\n",
       "  0.00010964445,\n",
       "  0.00054706435,\n",
       "  0.012179828,\n",
       "  0.00096995145,\n",
       "  0.00046662145,\n",
       "  0.012067362,\n",
       "  0.0009377614,\n",
       "  0.00094410457,\n",
       "  0.014941729],\n",
       " 'val_time_distributed_1_loss': [0.005834352,\n",
       "  0.2851868,\n",
       "  0.2983835,\n",
       "  0.19483474,\n",
       "  0.14486375,\n",
       "  0.17508802,\n",
       "  0.282197,\n",
       "  0.24634752,\n",
       "  0.27208486,\n",
       "  0.20647953],\n",
       " 'val_time_distributed_2_loss': [0.008067679,\n",
       "  0.06926195,\n",
       "  0.14195547,\n",
       "  0.10002588,\n",
       "  0.08014474,\n",
       "  0.09417976,\n",
       "  0.13698867,\n",
       "  0.14015377,\n",
       "  0.118722744,\n",
       "  0.07290939],\n",
       " 'val_time_distributed_3_loss': [0.001096112,\n",
       "  0.27341765,\n",
       "  0.08470181,\n",
       "  0.064896375,\n",
       "  0.060926937,\n",
       "  0.08908972,\n",
       "  0.06562367,\n",
       "  0.07988253,\n",
       "  0.10574251,\n",
       "  0.0505316],\n",
       " 'val_time_distributed_acc': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987],\n",
       " 'val_time_distributed_1_acc': [1.0,\n",
       "  0.9274,\n",
       "  0.8809,\n",
       "  0.9251,\n",
       "  0.937,\n",
       "  0.9162,\n",
       "  0.8761,\n",
       "  0.9081,\n",
       "  0.8927,\n",
       "  0.9277],\n",
       " 'val_time_distributed_2_acc': [0.9987,\n",
       "  0.986,\n",
       "  0.965,\n",
       "  0.975,\n",
       "  0.9765,\n",
       "  0.972,\n",
       "  0.9671,\n",
       "  0.9609,\n",
       "  0.9681,\n",
       "  0.9821],\n",
       " 'val_time_distributed_3_acc': [1.0,\n",
       "  0.9564,\n",
       "  0.9852,\n",
       "  0.99,\n",
       "  0.9741,\n",
       "  0.9652,\n",
       "  0.9896,\n",
       "  0.9805,\n",
       "  0.9705,\n",
       "  0.9923]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
