{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all helper functions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.models.data_utils import *\n",
    "from src.models.model_utils import *\n",
    "from src.models.train_model import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info:\n",
    "\n",
    "### Code structure\n",
    "- Helper functions are stored in src/models\n",
    "- Data is stored in data/processed\n",
    "\n",
    "### Corpora and annotation\n",
    "#### Dicta-Sign\n",
    "Annotations include (see more detail in the ortolang repo):\n",
    "* **fls**: fully-lexical signs, encoded as categorical values (gloss indices)\n",
    "* **PT**:    pointing signs, encoded as binary\n",
    "* **PT_PRO1**, **PT_PRO2**, **PT_PRO3**, **PT_LOC**, **PT_DET**, **PT_LBUOY**, **PT_BUOY**: sub-categories for pointing signs, encoded as binary\n",
    "* **DS**:    depicting signs, encoded as binary\n",
    "* **DSA**, **DSG**, **DSL**, **DSM**, **DSS**, **DST**, **DSX**: sub-categories for depicting signs, encoded as binary\n",
    "* **FBUOY**: fragment buoys, encoded as binary\n",
    "* **N**:     numbering signs, encoded as binary\n",
    "* **FS**:    fingerspelling signs, encoded as binary\n",
    "\n",
    "#### NCSLGR\n",
    "Annotations include (all data is binary):\n",
    "* **lexical_with_ns_not_fs**: lexical signs, including numbering signs but excluding fingerspelling signs\n",
    "* **fingerspelling**, **fingerspelled_loan_signs**: fingerspelling signs, finglerspelling loan signs\n",
    "* **IX_1p**, **IX_2p**, **IX_3p**, **IX_loc**: sub-categories for pointing signs\n",
    "* **POSS**, **SELF**: possessive pronouns\n",
    "* **DCL**, **LCL**, **SCL**, **BCL**, **ICL**, **BPCL**, **PCL**: sub-categories for classifier signs (i.e. depicting signs)\n",
    "* **gesture**: culturally shared gestures\n",
    "* **part_indef**\n",
    "* **other**\n",
    "\n",
    "### Type of input features\n",
    "Originally, this code was designed around preprocessed features for each frame. Possible features types are : \n",
    "- **2Draw**\n",
    "- **2Draw_HS**\n",
    "- **2Draw_HS_noOP**\n",
    "- **2Draw_noHands**\n",
    "- **2Dfeatures**\n",
    "- **2Dfeatures_HS**\n",
    "- **2Dfeatures_HS_noOP**\n",
    "- **2Dfeatures_noHands**\n",
    "- **3Draw**\n",
    "- **3Draw_HS**\n",
    "- **3Draw_HS_noOP**\n",
    "- **3Draw_noHands**\n",
    "- **3Dfeatures**\n",
    "- **3Dfeatures_HS**\n",
    "- **3Dfeatures_HS_noOP**\n",
    "- **3Dfeatures_noHands**\n",
    "\n",
    "which correspond to 2D or 3D data, raw OpenPose or preprocessed body and face data, including or excluding hand shape estimates, including or excluding OpenPose hand data.\n",
    "\n",
    "The main data function `get_data_concatenated` requires a features dictionary, which can be obtained with the `getFeaturesDict` function.\n",
    "\n",
    "Recently, we also added direct image input to the model, but it has not been tested thoroughly.\n",
    "\n",
    "### Model outputs\n",
    "The model can be trained for the recognition of:\n",
    "- one or several (N) mixed linguistic descriptors in parallel, possibly simultaneously true. Each descriptor includes a 'garbage/other' class.\n",
    "    - ex. 1 (N = 4) : Y1 = [Y1_1 : lexical signs (categorical with 4 different signs), Y1_2 : pointing signs, Y1_3 : depicting signs, Y1_4 : fragment buoys]\n",
    "    - ex. 2 (N = 1) : Y2 = [Y2_1 : pointing signs to PRO1/2/3]\n",
    "    - ex. 3 (N = 2) : Y3 = [Y3_1 : pointing signs to PRO1/2/3, Y3_2 : depicting signs A/G]\n",
    "    - ex. 4 (N = 1) : Y4 = [Y4_1 : depicting signs]\n",
    "- sign types, i.e. the most probable sign type for each frame. In this case, lexical signs are seen as binary.\n",
    "    - ex. 5: Y5 = [other, lexical signs,  pointing signs, depicting signs, fragment buoys]. \n",
    "    - ex. 6: Y6 = [other, depicting signs] : this should give the same results as ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help on a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function categorical_conversion_seq in module src.models.data_utils:\n",
      "\n",
      "categorical_conversion_seq(data, nonZeroCategories=[1])\n",
      "    Converting annotations of one type to categorical values (for a sequence or a video).\n",
      "    \n",
      "    Inputs:\n",
      "        data: a numpy array of annotations [time_steps, 1]\n",
      "        nonZeroCategories: list of meaningful annotation categories\n",
      "    \n",
      "    Outputs:\n",
      "        converted_data: categorical values of annotations [1, time_steps, categories]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help(function_name)\n",
    "# For instance:\n",
    "\n",
    "help(categorical_conversion_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for data handling (`data_utils.py`): `get_data_concatenated`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to extract data in usable format for training.\n",
    "\n",
    "`get_data_concatenated` returns [X_features, X_frames], Y, idx_trueData or [X_features, X_frames], Y (depending on return_idx_trueData)\n",
    "\n",
    "#### Outputs\n",
    "- **X_features** is a numpy array of size [1, total_time_steps, features_number] containing all retained preprocessed features for all retained frames\n",
    "- **X_frames** is simply a list of paths for all retained frames (frames cannot be stored in memory directly, and will have to be read during training thanks to frames paths)\n",
    "- **Y** is the annotation data (i.e. ground truth data) in the desired format\n",
    "\n",
    "#### Inputs\n",
    "- **corpus** (string)\n",
    "- **output_form**:\n",
    "    - 'mixed' if different and separated Outputs\n",
    "    - 'sign_types' if annotation is only a binary matrix of sign types\n",
    "- **output_names_final**: list of outputs (strings)\n",
    "- **output_categories_or_names_original**:\n",
    "    - if output_form: 'mixed': list of lists of meaningful annotation categories for each output\n",
    "    - if output_form: 'sign_types': list of lists of original names that are used to compose final outputs\n",
    "- **output_assemble**: used only if output_form: 'mixed'. List of lists of original names that can be assembled to compose final outputs, only considered if binary annotation category\n",
    "- **features_dict**: a dictionary indication which features to keep ; e.g.: {'features_HS':np.arange(0, 420), 'features_HS_norm':np.array([]), 'raw':np.array([]), 'raw_norm':np.array([])}\n",
    "- **preloaded_features**: if features are already loaded, in the format of a list (features for each video)\n",
    "- **preloaded_annotations**: if annotations are already loaded, in the format of a list of lists (for each output type, each video), categorical values\n",
    "- **video_indices**: numpy array for a list of videos\n",
    "- **separation**: in order to separate consecutive videos\n",
    "- **from_notebook**: if notebook script, data is in parent folder\n",
    "- **return_idx_trueData**: if True, returns a binary vector with 0 where separations are\n",
    "- **features_type**: 'features', 'frames', 'both'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for model handling (`model_utils.py`): `get_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to obtain the Keras model.\n",
    "\n",
    "`get_model` returns a Keras model\n",
    "\n",
    "#### Outputs\n",
    "- a Keras model\n",
    "\n",
    "#### Inputs\n",
    "- **output_names**: list of outputs (strings)\n",
    "- **output_classes**: list of number of classes of each output type\n",
    "- **output_weights**: list of weights for each_output\n",
    "- **conv** (bool): if True, applies convolution on input\n",
    "- **conv_filt**: number of convolution filters\n",
    "- **conv_ker**: size of convolution kernel\n",
    "- **conv_strides**: size of convolution strides\n",
    "- **rnn_number**: number of recurrent layers\n",
    "- **rnn_type**: type of recurrent layers (string)\n",
    "- **rnn_hidden_units**: number of hidden units\n",
    "- **dropout**: how much dropout (0 to 1)\n",
    "- **att_in_rnn**: if True, applies attention layer before recurrent layers\n",
    "- **att_in_rnn_single**: single (shared) attention layer or not\n",
    "- **att_in_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **att_out_rnn**: if True, applies attention layer after recurrent layers\n",
    "- **att_out_rnn_single**: single (shared) attention layer or not\n",
    "- **att_out_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **rnn_return_sequences**: if False, only last timestep of recurrent layers is returned\n",
    "- **classif_local** (bool): whether classification is for each timestep (local) of globally for the sequence\n",
    "- **mlp_layers_number**: number of additional dense layers\n",
    "- **mlp_layers_size**: size of additional dense layers\n",
    "- **optimizer**: gradient optimizer type (string)\n",
    "- **learning_rate**: learning rate (float)\n",
    "- **time_steps**: length of sequences (int)\n",
    "- **features_number**: number of features (int)\n",
    "- **features_type**: 'features' (1D vector of features), 'frames' (for a CNN processing) or 'both'\n",
    "- **img_height** and **img_width**: size of CNN input\n",
    "- **cnnType**: 'resnet', 'vgg' or 'mobilenet'\n",
    "- **cnnFirstTrainedLayer**: index of first trainable layer in CNN (int)\n",
    "- **cnnReduceDim**: if greater than 0, size of CNN flattened output is reduced to cnnReduceDim\n",
    "- **print_summary** (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building data and model together (first series of examples)\n",
    "\n",
    "In these examples we analyze the different types of output form Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 66\n",
      "Valid: 10\n",
      "Test: 18\n",
      "Total: 94\n"
     ]
    }
   ],
   "source": [
    "# let us split train/valid/test videos\n",
    "# In this case we split by signers in a manual fashion\n",
    "\n",
    "idxTrain, idxValid, idxTest = getVideoIndicesSplitDictaSign(tasksTrain=[],\n",
    "                                                            tasksValid=[],\n",
    "                                                            tasksTest=[],\n",
    "                                                            signersTrain=[0,1,2,3,4,5,6,7,8,9],\n",
    "                                                            signersValid=[10,11,12],\n",
    "                                                            signersTest=[13,14,15],\n",
    "                                                            excludeTask9=False,\n",
    "                                                            videoSplitMode='manual',\n",
    "                                                            checkSplits=True,\n",
    "                                                            checkSets=True,\n",
    "                                                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features_HS': array([], dtype=float64), 'features_HS_norm': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419]), 'raw': array([], dtype=float64), 'raw_norm': array([], dtype=float64), '2Dfeatures': array([], dtype=float64), '2Dfeatures_norm': array([], dtype=float64)}\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "# Getting a dictionary for desired preprocessed features\n",
    "# In this case we ask for normalized 3Dfeatures_HS (this correspond to a total number of 420 features):\n",
    "\n",
    "features_dict, features_number = getFeaturesDict(inputType='3Dfeatures_HS', inputNormed=True)\n",
    "\n",
    "print(features_dict)\n",
    "print(features_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `output_form = 'mixed'` (examples 1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "idGloss = {}\n",
    "\n",
    "with open('Dicta-Sign-LSF_ID.csv', newline='') as csvfile:\n",
    "    glossreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    for row in glossreader:\n",
    "        idGloss[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARIS3 (TOUR EIFFEL):NS\n",
      "RESTAURANT\n",
      "QUALITE-DE GRANDE /EXCELLENT\n",
      "VISITER1:VAR\n"
     ]
    }
   ],
   "source": [
    "# we only consider 4 lexical signs, indices 41891,43413,43422,42992\n",
    "\n",
    "flsKept = [41891,43413,43422,42992]\n",
    "\n",
    "# that correspond to glosses:\n",
    "\n",
    "for i in flsKept:\n",
    "    print(idGloss[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_1, X_frames_train_1], Y_train_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            output_names_final=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                            output_categories_or_names_original=[flsKept, [1], [1], [1]],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1.shape)\n",
    "print(X_frames_train_1.shape)\n",
    "print(len(Y_train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, X_feat is a big matrix storing all 420 preprocessed features for each frame. We can print the first 15 features for frame number 192:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.35301688e-01 1.29614295e-02 2.68831011e-02 3.30785895e-03\n",
      " 1.42225486e-04 1.38188677e-03 2.03026459e-02 2.27585621e-03\n",
      " 1.62972094e-04 2.75143277e-04 1.34467043e-03 6.37093792e-03\n",
      " 6.20781793e-05 3.47524241e-04 8.72815102e-02]\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1[0,192,0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_frames stores paths for all frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localHD/DictaSign/convert/img/DictaSign_lsf_S5_T1_A9_front/00193.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_frames_train_1[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train stores the 4 linguistic descriptors annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 5)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0].shape)\n",
    "print(Y_train_1[1].shape)\n",
    "print(Y_train_1[2].shape)\n",
    "print(Y_train_1[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0][0,192,:])\n",
    "print(Y_train_1[1][0,192,:])\n",
    "print(Y_train_1[2][0,192,:])\n",
    "print(Y_train_1[3][0,192,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame 192 is annotated as non-lexical, depicting, non-pointing, non-fbuoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 100, 200)     252200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 110)     112640      conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 110)     73040       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 5)       555         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 439,101\n",
      "Trainable params: 439,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# training model can be built to take preprocessed features as input, frames as input or both\n",
    "model_1_features = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[5,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 100, 2048)    23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 110)     925760      time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 110)     73040       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 5)       555         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,587,733\n",
      "Trainable params: 5,465,685\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_frames = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[5,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 200)     252200      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 100, 2048)    23587712    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_frames (Concaten (None, 100, 2248)    0           conv1d_1[0][0]                   \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100, 110)     1013760     merge_features_frames[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 100, 110)     73040       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 100, 5)       555         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,927,933\n",
      "Trainable params: 5,805,885\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_both = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[5,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category\n",
    "[X_feat_train_2, X_frames_train_2], Y_train_2 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            output_names_final=['pointing-signs-pro123'],\n",
    "                            output_categories_or_names_original=[[1]],\n",
    "                            output_assemble=[['PT_PRO1','PT_PRO2', 'PT_PRO3']],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_2.shape)\n",
    "print(X_frames_train_2.shape)\n",
    "print(len(Y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_2[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2_features = get_model(output_names=['pointing-signs-pro123'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category\n",
    "# and data from 2 channels (DSA, DSG) are assembled to form a second category\n",
    "[X_feat_train_3, X_frames_train_3], Y_train_3 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            output_names_final=['pointing-signs-pro123', 'depicting-signs-AG'],\n",
    "                            output_categories_or_names_original=[[1],[1]],\n",
    "                            output_assemble=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['DSA', 'DSG']],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_3.shape)\n",
    "print(X_frames_train_3.shape)\n",
    "print(len(Y_train_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_3[0].shape)\n",
    "print(Y_train_3[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 100, 200)     252200      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 100, 110)     112640      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 100, 110)     73040       bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 438,324\n",
      "Trainable params: 438,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_3_features = get_model(output_names=['pointing-signs-pro123', 'depicting-signs-AG'],\n",
    "                    output_classes=[2, 2],\n",
    "                    output_weights=[1, 1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, only depicting signs as output\n",
    "[X_feat_train_4, X_frames_train_4], Y_train_4 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            output_names_final=['DS'],\n",
    "                            output_categories_or_names_original=[[1]],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_feat_train_4.shape)\n",
    "print(X_frames_train_4.shape)\n",
    "print(len(Y_train_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train_4[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
