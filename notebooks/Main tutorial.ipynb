{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all helper functions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.models.data_utils import *\n",
    "from src.models.model_utils import *\n",
    "from src.models.train_model import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info:\n",
    "\n",
    "### Code structure\n",
    "- Helper functions are stored in src/models\n",
    "- Data is stored in data/processed\n",
    "\n",
    "### Corpora and annotation\n",
    "#### Dicta-Sign\n",
    "Annotations include (see more detail in the ortolang repo):\n",
    "* **fls**: fully-lexical signs, encoded as categorical values (gloss indices)\n",
    "* **PT**:    pointing signs, encoded as binary\n",
    "* **PT_PRO1**, **PT_PRO2**, **PT_PRO3**, **PT_LOC**, **PT_DET**, **PT_LBUOY**, **PT_BUOY**: sub-categories for pointing signs, encoded as binary\n",
    "* **DS**:    depicting signs, encoded as binary\n",
    "* **DSA**, **DSG**, **DSL**, **DSM**, **DSS**, **DST**, **DSX**: sub-categories for depicting signs, encoded as binary\n",
    "* **FBUOY**: fragment buoys, encoded as binary\n",
    "* **N**:     numbering signs, encoded as binary\n",
    "* **FS**:    fingerspelling signs, encoded as binary\n",
    "\n",
    "#### NCSLGR\n",
    "Annotations include (all data is binary):\n",
    "* **lexical_with_ns_not_fs**: lexical signs, including numbering signs but excluding fingerspelling signs\n",
    "* **fingerspelling**, **fingerspelled_loan_signs**: fingerspelling signs, finglerspelling loan signs\n",
    "* **IX_1p**, **IX_2p**, **IX_3p**, **IX_loc**: sub-categories for pointing signs\n",
    "* **POSS**, **SELF**: possessive pronouns\n",
    "* **DCL**, **LCL**, **SCL**, **BCL**, **ICL**, **BPCL**, **PCL**: sub-categories for classifier signs (i.e. depicting signs)\n",
    "* **gesture**: culturally shared gestures\n",
    "* **part_indef**\n",
    "* **other**\n",
    "\n",
    "### Type of input features\n",
    "Originally, this code was designed around preprocessed features for each frame. Possible features types are : \n",
    "- **2Draw**\n",
    "- **2Draw_HS**\n",
    "- **2Draw_HS_noOP**\n",
    "- **2Draw_noHands**\n",
    "- **2Dfeatures**\n",
    "- **2Dfeatures_HS**\n",
    "- **2Dfeatures_HS_noOP**\n",
    "- **2Dfeatures_noHands**\n",
    "- **3Draw**\n",
    "- **3Draw_HS**\n",
    "- **3Draw_HS_noOP**\n",
    "- **3Draw_noHands**\n",
    "- **3Dfeatures**\n",
    "- **3Dfeatures_HS**\n",
    "- **3Dfeatures_HS_noOP**\n",
    "- **3Dfeatures_noHands**\n",
    "\n",
    "which correspond to 2D or 3D data, raw OpenPose or preprocessed body and face data, including or excluding hand shape estimates, including or excluding OpenPose hand data.\n",
    "\n",
    "The main data function `get_data_concatenated` requires a features dictionary, which can be obtained with the `getFeaturesDict` function.\n",
    "\n",
    "Recently, we also added direct image input to the model, but it has not been tested thoroughly.\n",
    "\n",
    "### Model outputs\n",
    "The model can be trained for the recognition of:\n",
    "- one or several (N) mixed linguistic descriptors in parallel, possibly simultaneously true. Each descriptor includes a 'garbage/other' class.\n",
    "    - **ex. 1** (N = 4) : Y1 = [Y1_1 : lexical signs (categorical with 4 different signs), Y1_2 : pointing signs, Y1_3 : depicting signs, Y1_4 : fragment buoys]\n",
    "    - **ex. 2** (N = 2) : Y2 = [Y2_1 : pointing signs to PRO1/2/3, Y2_2 : lexical signs (all)]\n",
    "    - **ex. 3** (N = 2) : Y3 = [Y3_1 : pointing signs to PRO1/2/3, Y3_2 : depicting signs A/G]\n",
    "    - **ex. 4** (N = 1) : Y4 = [Y4_1 : depicting signs]\n",
    "- sign types, i.e. the most probable sign type for each frame. In this case, lexical signs are seen as binary.\n",
    "    - **ex. 5**: Y5 = [other, lexical signs,  pointing signs, depicting signs, fragment buoys]. \n",
    "    - **ex. 6**: Y6 = [other, depicting signs] : this should give the same results as ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help on a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_raw_annotation_from_file in module src.models.data_utils:\n",
      "\n",
      "get_raw_annotation_from_file(corpus, from_notebook=False)\n",
      "    Gets raw annotation from data file\n",
      "    \n",
      "    Inputs:\n",
      "        corpus: 'DictaSign' or 'NCSLGR'\n",
      "        from_notebook: True if used in Jupyter notebook\n",
      "    \n",
      "    Outputs:\n",
      "        Annotation data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help(function_name)\n",
    "# For instance:\n",
    "\n",
    "help(get_raw_annotation_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for data handling (`data_utils.py`): `get_data_concatenated`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to extract data in usable format for training.\n",
    "\n",
    "`get_data_concatenated` returns [X_features, X_frames], Y, idx_trueData or [X_features, X_frames], Y (depending on return_idx_trueData)\n",
    "\n",
    "#### Outputs\n",
    "- **X_features** is a numpy array of size [1, total_time_steps, features_number] containing all retained preprocessed features for all retained frames\n",
    "- **X_frames** is simply a list of paths for all retained frames (frames cannot be stored in memory directly, and will have to be read during training thanks to frames paths)\n",
    "- **Y** is the annotation data (i.e. ground truth data) in the desired format\n",
    "\n",
    "#### Inputs\n",
    "- **corpus** (string)\n",
    "- **output_form**:\n",
    "    - 'mixed' if different and separated Outputs\n",
    "    - 'sign_types' if annotation is only a binary matrix of sign types\n",
    "- **types**: a list of lists of original names that are used to compose final outputs\n",
    "- **nonZero**: a list of lists of nonzero values to consider. If 4 outputs with all nonZero values should be considered, nonZero=[[],[],[],[]]\n",
    "- **binary**: only considered when output_form=mixed. It's a list (True/False) indicating whether the values should be categorical or binary\n",
    "- **features_dict**: a dictionary indication which features to keep ; e.g.: {'features_HS':np.arange(0, 420), 'features_HS_norm':np.array([]), 'raw':np.array([]), 'raw_norm':np.array([])}\n",
    "- **preloaded_features**: if features are already loaded, in the format of a list (features for each video)\n",
    "- **provided_annotation**: raw annotation data (not needed)\n",
    "- **video_indices**: numpy array for a list of videos\n",
    "- **separation**: in order to separate consecutive videos\n",
    "- **from_notebook**: if notebook script, data is in parent folder\n",
    "- **return_idx_trueData**: if True, returns a binary vector with 0 where separations are\n",
    "- **features_type**: 'features', 'frames', 'both'            \n",
    "- **frames_path_before_video**: video frames are supposed to be in folders, like '/localHD/DictaSign/convert/img/DictaSign_lsf_S7_T2_A10',\n",
    "- **empty_image_path**: path of a white frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main helper function for model handling (`model_utils.py`): `get_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function, which enables to obtain the Keras model.\n",
    "\n",
    "`get_model` returns a Keras model\n",
    "\n",
    "#### Outputs\n",
    "- a Keras model\n",
    "\n",
    "#### Inputs\n",
    "- **output_names**: list of outputs (strings)\n",
    "- **output_classes**: list of number of classes of each output type\n",
    "- **output_weights**: list of weights for each_output\n",
    "- **conv** (bool): if True, applies convolution on input\n",
    "- **conv_filt**: number of convolution filters\n",
    "- **conv_ker**: size of convolution kernel\n",
    "- **conv_strides**: size of convolution strides\n",
    "- **rnn_number**: number of recurrent layers\n",
    "- **rnn_type**: type of recurrent layers (string)\n",
    "- **rnn_hidden_units**: number of hidden units\n",
    "- **dropout**: how much dropout (0 to 1)\n",
    "- **att_in_rnn**: if True, applies attention layer before recurrent layers\n",
    "- **att_in_rnn_single**: single (shared) attention layer or not\n",
    "- **att_in_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **att_out_rnn**: if True, applies attention layer after recurrent layers\n",
    "- **att_out_rnn_single**: single (shared) attention layer or not\n",
    "- **att_out_rnn_type** (string): timewise or featurewise attention layer\n",
    "- **rnn_return_sequences**: if False, only last timestep of recurrent layers is returned\n",
    "- **classif_local** (bool): whether classification is for each timestep (local) of globally for the sequence\n",
    "- **mlp_layers_number**: number of additional dense layers\n",
    "- **mlp_layers_size**: size of additional dense layers\n",
    "- **optimizer**: gradient optimizer type (string)\n",
    "- **learning_rate**: learning rate (float)\n",
    "- **time_steps**: length of sequences (int)\n",
    "- **features_number**: number of features (int)\n",
    "- **features_type**: 'features' (1D vector of features), 'frames' (for a CNN processing) or 'both'\n",
    "- **img_height** and **img_width**: size of CNN input\n",
    "- **cnnType**: 'resnet', 'vgg' or 'mobilenet'\n",
    "- **cnnFirstTrainedLayer**: index of first trainable layer in CNN (int)\n",
    "- **cnnReduceDim**: if greater than 0, size of CNN flattened output is reduced to cnnReduceDim\n",
    "- **print_summary** (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building data and model together \n",
    "\n",
    "In these examples we analyze the different types of output form Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:\n",
      "Train: 66\n",
      "Valid: 10\n",
      "Test: 18\n",
      "Total: 94\n"
     ]
    }
   ],
   "source": [
    "# let us split train/valid/test videos\n",
    "# In this case we split by signers in a manual fashion\n",
    "\n",
    "idxTrain, idxValid, idxTest = getVideoIndicesSplitDictaSign(tasksTrain=[],\n",
    "                                                            tasksValid=[],\n",
    "                                                            tasksTest=[],\n",
    "                                                            signersTrain=[0,1,2,3,4,5,6,7,8,9],\n",
    "                                                            signersValid=[10,11,12],\n",
    "                                                            signersTest=[13,14,15],\n",
    "                                                            excludeTask9=False,\n",
    "                                                            videoSplitMode='manual',\n",
    "                                                            checkSplits=True,\n",
    "                                                            checkSets=True,\n",
    "                                                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features_HS': array([], dtype=float64), 'features_HS_norm': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419]), 'raw': array([], dtype=float64), 'raw_norm': array([], dtype=float64), '2Dfeatures': array([], dtype=float64), '2Dfeatures_norm': array([], dtype=float64)}\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "# Getting a dictionary for desired preprocessed features\n",
    "# In this case we ask for normalized 3Dfeatures_HS (this correspond to a total number of 420 features):\n",
    "\n",
    "features_dict, features_number = getFeaturesDict(inputType='3Dfeatures_HS', inputNormed=True)\n",
    "\n",
    "print(features_dict)\n",
    "print(features_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First category of examples: `output_form = 'mixed'` (examples 1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "idGloss = {}\n",
    "\n",
    "with open('Dicta-Sign-LSF_ID.csv', newline='') as csvfile:\n",
    "    glossreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    for row in glossreader:\n",
    "        idGloss[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARIS3 (TOUR EIFFEL):NS\n",
      "RESTAURANT\n",
      "QUALITE-DE GRANDE /EXCELLENT\n",
      "VISITER1:VAR\n"
     ]
    }
   ],
   "source": [
    "# we only consider 4 lexical signs, indices 41891,43413,43422,42992\n",
    "\n",
    "flsKept = [41891,43413,43422,42992]\n",
    "N_fls = len(flsKept)\n",
    "\n",
    "# that correspond to glosses:\n",
    "\n",
    "for i in flsKept:\n",
    "    print(idGloss[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_1, X_frames_train_1], Y_train_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1.shape)\n",
    "print(X_frames_train_1.shape)\n",
    "print(len(Y_train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, X_feat is a big matrix storing all 420 preprocessed features for each frame. We can print the first 15 features for frame number 192:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.56896496e-01 8.08442011e-02 6.51594326e-02 4.68903668e-02\n",
      " 7.92527862e-05 2.17038882e-03 3.29143912e-01 1.75906322e-03\n",
      " 2.52671860e-04 2.70878343e-04 4.83787793e-04 4.01530191e-02\n",
      " 3.07233859e-04 4.62686792e-02 4.28746128e-03]\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_1[0,192,0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_frames stores paths for all frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localHD/DictaSign/convert/img/DictaSign_lsf_S4_T8_B14_front/00193.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_frames_train_1[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train stores the 4 linguistic descriptors annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 5)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0].shape)\n",
    "print(Y_train_1[1].shape)\n",
    "print(Y_train_1[2].shape)\n",
    "print(Y_train_1[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1916\n"
     ]
    }
   ],
   "source": [
    "# First depicting frame:\n",
    "i_DS_one = np.where(Y_train_1[1][0,:,1]==1)[0][0]\n",
    "print(i_DS_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_1[0][0,i_DS_one,:])\n",
    "print(Y_train_1[1][0,i_DS_one,:])\n",
    "print(Y_train_1[2][0,i_DS_one,:])\n",
    "print(Y_train_1[3][0,i_DS_one,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame 192 is annotated as non-lexical, depicting, non-pointing, non-fbuoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 100, 200)     252200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 110)     112640      conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 110)     73040       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 5)       555         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 2)       222         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 439,101\n",
      "Trainable params: 439,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# training model can be built to take preprocessed features as input, frames as input or both\n",
    "model_1_features = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 100, 2048)    23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 110)     925760      time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 110)     73040       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 5)       555         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 100, 2)       222         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,587,733\n",
      "Trainable params: 5,465,685\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_frames = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 100, 224, 22 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 200)     252200      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 100, 2048)    23587712    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features_frames (Concaten (None, 100, 2248)    0           conv1d_1[0][0]                   \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100, 110)     1013760     merge_features_frames[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 100, 110)     73040       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 100, 5)       555         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 100, 2)       222         bidirectional_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,927,933\n",
      "Trainable params: 5,805,885\n",
      "Non-trainable params: 19,122,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_both = get_model(output_names=['fls', 'DS', 'PT', 'FBUOY'],\n",
    "                    output_classes=[N_fls+1,2,2,2],\n",
    "                    output_weights=[1,1,1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category, \n",
    "# and a second category corresponds to all lexical signs (binary)\n",
    "[X_feat_train_2, X_frames_train_2], Y_train_2 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['fls']],\n",
    "                            nonZero=[[],[]],\n",
    "                            binary=[True,True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_2.shape)\n",
    "print(X_frames_train_2.shape)\n",
    "print(len(Y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_2[0].shape)\n",
    "print(Y_train_2[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 200)     252200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 100, 110)     112640      conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 100, 110)     73040       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 100, 2)       222         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 100, 2)       222         bidirectional_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 438,324\n",
      "Trainable params: 438,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_2_features = get_model(output_names=['pointing-signs-pro123', 'lexical'],\n",
    "                    output_classes=[2,2],\n",
    "                    output_weights=[1,1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, data from 3 channels (PT_PRO1, 2 and 3) are assembled to form a new category\n",
    "# and data from 2 channels (DSA, DSG) are assembled to form a second category\n",
    "[X_feat_train_3, X_frames_train_3], Y_train_3 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['PT_PRO1','PT_PRO2', 'PT_PRO3'], ['DSA', 'DSG']],\n",
    "                            nonZero=[[],[]],\n",
    "                            binary=[True,True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_3.shape)\n",
    "print(X_frames_train_3.shape)\n",
    "print(len(Y_train_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_3[0].shape)\n",
    "print(Y_train_3[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 100, 420)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 100, 200)     252200      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 100, 110)     112640      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 100, 110)     73040       bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 100, 2)       222         bidirectional_9[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 438,324\n",
      "Trainable params: 438,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_3_features = get_model(output_names=['pointing-signs-pro123', 'depicting-signs-AG'],\n",
    "                    output_classes=[2, 2],\n",
    "                    output_weights=[1, 1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, only depicting signs as output\n",
    "[X_feat_train_4, X_frames_train_4], Y_train_4 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[True],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_4.shape)\n",
    "print(X_frames_train_4.shape)\n",
    "print(len(Y_train_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 2)\n",
      "[[623477.  42135.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_4[0].shape)\n",
    "print(np.sum(Y_train_4[0],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_4_features = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second category of examples: `output_form = 'sign_types'` (examples 5 and 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 5 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_5, X_frames_train_5], Y_train_5 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['fls'],['DS'],['PT'],['FBUOY']],\n",
    "                            nonZero=[[],[],[],[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "(1, 665612, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_5.shape)\n",
    "print(X_frames_train_5.shape)\n",
    "print(Y_train_5.shape) # when output_form='sign_types', Y is not a list, but a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[488403. 126592.  42135.  15146.   9260.]]\n"
     ]
    }
   ],
   "source": [
    "#print(Y_train_5[0,0:200,:])\n",
    "print(np.sum(Y_train_5,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 100, 5)            555       \n",
      "=================================================================\n",
      "Total params: 438,435\n",
      "Trainable params: 438,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_5_features = get_model(output_names=['fls-DS-PT-FBUOY'],\n",
    "                    output_classes=[5],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex. 6 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_train_6, X_frames_train_6], Y_train_6 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='sign_types',\n",
    "                            types=[['DS']],\n",
    "                            nonZero=[[]],\n",
    "                            binary=[],\n",
    "                            video_indices=idxTrain,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 665612, 420)\n",
      "(665612,)\n",
      "(1, 665612, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_feat_train_6.shape)\n",
    "print(X_frames_train_6.shape)\n",
    "print(Y_train_6.shape) # when output_form='sign_types', Y is not a list, but a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[623477.  42135.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_6[0,1000:1020,:])\n",
    "print(np.sum(Y_train_6,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 100, 420)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 200)          252200    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 100, 110)          112640    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 100, 110)          73040     \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 100, 2)            222       \n",
      "=================================================================\n",
      "Total params: 438,102\n",
      "Trainable params: 438,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using only preprocessed features as input:\n",
    "model_6_features = get_model(output_names=['DS'],\n",
    "                    output_classes=[2],\n",
    "                    output_weights=[1],\n",
    "                    features_number=features_number,\n",
    "                    features_type='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "Now we look at how to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs=10\n",
    "seq_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_feat_valid_1, X_frames_valid_1], Y_valid_1 =\\\n",
    "      get_data_concatenated(corpus='DictaSign',\n",
    "                            output_form='mixed',\n",
    "                            types=[['fls'], ['DS'], ['PT'], ['FBUOY']],\n",
    "                            nonZero=[flsKept, [], [], []],\n",
    "                            binary=[False, True, True, True],\n",
    "                            video_indices=idxValid,\n",
    "                            features_dict=features_dict,\n",
    "                            features_type='both',\n",
    "                            from_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 67.0 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 41s 614ms/step - loss: 0.4873 - time_distributed_loss: 0.0378 - time_distributed_1_loss: 0.2391 - time_distributed_2_loss: 0.1289 - time_distributed_3_loss: 0.0816 - time_distributed_acc: 0.9846 - time_distributed_1_acc: 0.9148 - time_distributed_2_acc: 0.9678 - time_distributed_3_acc: 0.9799 - val_loss: 0.0153 - val_time_distributed_loss: 2.9153e-04 - val_time_distributed_1_loss: 0.0058 - val_time_distributed_2_loss: 0.0081 - val_time_distributed_3_loss: 0.0011 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 1.0000 - val_time_distributed_2_acc: 0.9987 - val_time_distributed_3_acc: 1.0000\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 35s 516ms/step - loss: 0.3635 - time_distributed_loss: 0.0075 - time_distributed_1_loss: 0.1865 - time_distributed_2_loss: 0.0963 - time_distributed_3_loss: 0.0732 - time_distributed_acc: 0.9991 - time_distributed_1_acc: 0.9343 - time_distributed_2_acc: 0.9772 - time_distributed_3_acc: 0.9830 - val_loss: 0.6280 - val_time_distributed_loss: 1.0964e-04 - val_time_distributed_1_loss: 0.2852 - val_time_distributed_2_loss: 0.0693 - val_time_distributed_3_loss: 0.2734 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9274 - val_time_distributed_2_acc: 0.9860 - val_time_distributed_3_acc: 0.9564\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 38s 567ms/step - loss: 0.3096 - time_distributed_loss: 0.0047 - time_distributed_1_loss: 0.1663 - time_distributed_2_loss: 0.0872 - time_distributed_3_loss: 0.0514 - time_distributed_acc: 0.9994 - time_distributed_1_acc: 0.9381 - time_distributed_2_acc: 0.9785 - time_distributed_3_acc: 0.9884 - val_loss: 0.5256 - val_time_distributed_loss: 5.4706e-04 - val_time_distributed_1_loss: 0.2984 - val_time_distributed_2_loss: 0.1420 - val_time_distributed_3_loss: 0.0847 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.8809 - val_time_distributed_2_acc: 0.9650 - val_time_distributed_3_acc: 0.9852\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 41s 606ms/step - loss: 0.3050 - time_distributed_loss: 0.0103 - time_distributed_1_loss: 0.1583 - time_distributed_2_loss: 0.0793 - time_distributed_3_loss: 0.0570 - time_distributed_acc: 0.9987 - time_distributed_1_acc: 0.9382 - time_distributed_2_acc: 0.9794 - time_distributed_3_acc: 0.9854 - val_loss: 0.3719 - val_time_distributed_loss: 0.0122 - val_time_distributed_1_loss: 0.1948 - val_time_distributed_2_loss: 0.1000 - val_time_distributed_3_loss: 0.0649 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.9251 - val_time_distributed_2_acc: 0.9750 - val_time_distributed_3_acc: 0.9900\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.3225 - time_distributed_loss: 0.0080 - time_distributed_1_loss: 0.1597 - time_distributed_2_loss: 0.0908 - time_distributed_3_loss: 0.0640 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9356 - time_distributed_2_acc: 0.9741 - time_distributed_3_acc: 0.9807 - val_loss: 0.2869 - val_time_distributed_loss: 9.6995e-04 - val_time_distributed_1_loss: 0.1449 - val_time_distributed_2_loss: 0.0801 - val_time_distributed_3_loss: 0.0609 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9370 - val_time_distributed_2_acc: 0.9765 - val_time_distributed_3_acc: 0.9741\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 38s 563ms/step - loss: 0.2328 - time_distributed_loss: 0.0077 - time_distributed_1_loss: 0.1178 - time_distributed_2_loss: 0.0672 - time_distributed_3_loss: 0.0401 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9557 - time_distributed_2_acc: 0.9806 - time_distributed_3_acc: 0.9888 - val_loss: 0.3588 - val_time_distributed_loss: 4.6662e-04 - val_time_distributed_1_loss: 0.1751 - val_time_distributed_2_loss: 0.0942 - val_time_distributed_3_loss: 0.0891 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9162 - val_time_distributed_2_acc: 0.9720 - val_time_distributed_3_acc: 0.9652\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 36s 542ms/step - loss: 0.2762 - time_distributed_loss: 0.0067 - time_distributed_1_loss: 0.1452 - time_distributed_2_loss: 0.0812 - time_distributed_3_loss: 0.0431 - time_distributed_acc: 0.9991 - time_distributed_1_acc: 0.9425 - time_distributed_2_acc: 0.9757 - time_distributed_3_acc: 0.9885 - val_loss: 0.4969 - val_time_distributed_loss: 0.0121 - val_time_distributed_1_loss: 0.2822 - val_time_distributed_2_loss: 0.1370 - val_time_distributed_3_loss: 0.0656 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.8761 - val_time_distributed_2_acc: 0.9671 - val_time_distributed_3_acc: 0.9896\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 37s 548ms/step - loss: 0.2753 - time_distributed_loss: 0.0079 - time_distributed_1_loss: 0.1408 - time_distributed_2_loss: 0.0827 - time_distributed_3_loss: 0.0438 - time_distributed_acc: 0.9989 - time_distributed_1_acc: 0.9463 - time_distributed_2_acc: 0.9740 - time_distributed_3_acc: 0.9879 - val_loss: 0.4673 - val_time_distributed_loss: 9.3776e-04 - val_time_distributed_1_loss: 0.2463 - val_time_distributed_2_loss: 0.1402 - val_time_distributed_3_loss: 0.0799 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.9081 - val_time_distributed_2_acc: 0.9609 - val_time_distributed_3_acc: 0.9805\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 36s 534ms/step - loss: 0.2331 - time_distributed_loss: 0.0069 - time_distributed_1_loss: 0.1176 - time_distributed_2_loss: 0.0667 - time_distributed_3_loss: 0.0418 - time_distributed_acc: 0.9989 - time_distributed_1_acc: 0.9557 - time_distributed_2_acc: 0.9800 - time_distributed_3_acc: 0.9867 - val_loss: 0.4975 - val_time_distributed_loss: 9.4410e-04 - val_time_distributed_1_loss: 0.2721 - val_time_distributed_2_loss: 0.1187 - val_time_distributed_3_loss: 0.1057 - val_time_distributed_acc: 1.0000 - val_time_distributed_1_acc: 0.8927 - val_time_distributed_2_acc: 0.9681 - val_time_distributed_3_acc: 0.9705\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.2248 - time_distributed_loss: 0.0044 - time_distributed_1_loss: 0.1232 - time_distributed_2_loss: 0.0610 - time_distributed_3_loss: 0.0361 - time_distributed_acc: 0.9990 - time_distributed_1_acc: 0.9533 - time_distributed_2_acc: 0.9805 - time_distributed_3_acc: 0.9896 - val_loss: 0.3449 - val_time_distributed_loss: 0.0149 - val_time_distributed_1_loss: 0.2065 - val_time_distributed_2_loss: 0.0729 - val_time_distributed_3_loss: 0.0505 - val_time_distributed_acc: 0.9987 - val_time_distributed_1_acc: 0.9277 - val_time_distributed_2_acc: 0.9821 - val_time_distributed_3_acc: 0.9923\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model_1_features,\n",
    "                      [X_feat_train_1, X_frames_train_1],\n",
    "                      Y_train_1,\n",
    "                      [X_feat_valid_1, X_frames_valid_1],\n",
    "                      Y_valid_1,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4873052234969922,\n",
       "  0.36351746926779177,\n",
       "  0.3095500692169168,\n",
       "  0.3050421726014187,\n",
       "  0.3225172839836398,\n",
       "  0.23277043945976159,\n",
       "  0.27617699491666325,\n",
       "  0.2752775760617719,\n",
       "  0.23305533198055936,\n",
       "  0.22476121366246424],\n",
       " 'time_distributed_loss': [0.037806444,\n",
       "  0.0075106374,\n",
       "  0.0046897423,\n",
       "  0.010325068,\n",
       "  0.0080221575,\n",
       "  0.0076571107,\n",
       "  0.0066989427,\n",
       "  0.007943476,\n",
       "  0.006906639,\n",
       "  0.0044149715],\n",
       " 'time_distributed_1_loss': [0.23907705,\n",
       "  0.18651846,\n",
       "  0.16627303,\n",
       "  0.15832663,\n",
       "  0.15969767,\n",
       "  0.11779289,\n",
       "  0.14515437,\n",
       "  0.14083445,\n",
       "  0.11762877,\n",
       "  0.12321183],\n",
       " 'time_distributed_2_loss': [0.12885219,\n",
       "  0.09632742,\n",
       "  0.08722829,\n",
       "  0.07934722,\n",
       "  0.09076402,\n",
       "  0.06719639,\n",
       "  0.081218965,\n",
       "  0.08266508,\n",
       "  0.06670073,\n",
       "  0.061022077],\n",
       " 'time_distributed_3_loss': [0.081569545,\n",
       "  0.07316094,\n",
       "  0.05135904,\n",
       "  0.057043206,\n",
       "  0.06403344,\n",
       "  0.040124036,\n",
       "  0.0431047,\n",
       "  0.04383459,\n",
       "  0.041819192,\n",
       "  0.036112316],\n",
       " 'time_distributed_acc': [0.9846254,\n",
       "  0.99911195,\n",
       "  0.9994433,\n",
       "  0.9987075,\n",
       "  0.99895376,\n",
       "  0.99903136,\n",
       "  0.9990791,\n",
       "  0.99894476,\n",
       "  0.9989239,\n",
       "  0.9990403],\n",
       " 'time_distributed_1_acc': [0.91484773,\n",
       "  0.9342851,\n",
       "  0.9380612,\n",
       "  0.9382,\n",
       "  0.9355597,\n",
       "  0.9557433,\n",
       "  0.9424582,\n",
       "  0.94625074,\n",
       "  0.9556776,\n",
       "  0.95326716],\n",
       " 'time_distributed_2_acc': [0.9678045,\n",
       "  0.9771687,\n",
       "  0.9785299,\n",
       "  0.979406,\n",
       "  0.9740806,\n",
       "  0.98060894,\n",
       "  0.9756522,\n",
       "  0.9740224,\n",
       "  0.98003435,\n",
       "  0.98050594],\n",
       " 'time_distributed_3_acc': [0.9799134,\n",
       "  0.98297316,\n",
       "  0.98841345,\n",
       "  0.9854448,\n",
       "  0.9806746,\n",
       "  0.9888418,\n",
       "  0.98851943,\n",
       "  0.9879418,\n",
       "  0.9867119,\n",
       "  0.9895582],\n",
       " 'val_loss': [0.01528966799378395,\n",
       "  0.6279760599136353,\n",
       "  0.525587797164917,\n",
       "  0.3719367980957031,\n",
       "  0.2869053781032562,\n",
       "  0.35882410407066345,\n",
       "  0.49687668681144714,\n",
       "  0.46732157468795776,\n",
       "  0.4974942207336426,\n",
       "  0.3448622524738312],\n",
       " 'val_time_distributed_loss': [0.00029152574,\n",
       "  0.00010964445,\n",
       "  0.00054706435,\n",
       "  0.012179828,\n",
       "  0.00096995145,\n",
       "  0.00046662145,\n",
       "  0.012067362,\n",
       "  0.0009377614,\n",
       "  0.00094410457,\n",
       "  0.014941729],\n",
       " 'val_time_distributed_1_loss': [0.005834352,\n",
       "  0.2851868,\n",
       "  0.2983835,\n",
       "  0.19483474,\n",
       "  0.14486375,\n",
       "  0.17508802,\n",
       "  0.282197,\n",
       "  0.24634752,\n",
       "  0.27208486,\n",
       "  0.20647953],\n",
       " 'val_time_distributed_2_loss': [0.008067679,\n",
       "  0.06926195,\n",
       "  0.14195547,\n",
       "  0.10002588,\n",
       "  0.08014474,\n",
       "  0.09417976,\n",
       "  0.13698867,\n",
       "  0.14015377,\n",
       "  0.118722744,\n",
       "  0.07290939],\n",
       " 'val_time_distributed_3_loss': [0.001096112,\n",
       "  0.27341765,\n",
       "  0.08470181,\n",
       "  0.064896375,\n",
       "  0.060926937,\n",
       "  0.08908972,\n",
       "  0.06562367,\n",
       "  0.07988253,\n",
       "  0.10574251,\n",
       "  0.0505316],\n",
       " 'val_time_distributed_acc': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9987],\n",
       " 'val_time_distributed_1_acc': [1.0,\n",
       "  0.9274,\n",
       "  0.8809,\n",
       "  0.9251,\n",
       "  0.937,\n",
       "  0.9162,\n",
       "  0.8761,\n",
       "  0.9081,\n",
       "  0.8927,\n",
       "  0.9277],\n",
       " 'val_time_distributed_2_acc': [0.9987,\n",
       "  0.986,\n",
       "  0.965,\n",
       "  0.975,\n",
       "  0.9765,\n",
       "  0.972,\n",
       "  0.9671,\n",
       "  0.9609,\n",
       "  0.9681,\n",
       "  0.9821],\n",
       " 'val_time_distributed_3_acc': [1.0,\n",
       "  0.9564,\n",
       "  0.9852,\n",
       "  0.99,\n",
       "  0.9741,\n",
       "  0.9652,\n",
       "  0.9896,\n",
       "  0.9805,\n",
       "  0.9705,\n",
       "  0.9923]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
